{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fcb594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "#change to any directory you have to store the repo\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "\n",
    "from src.features.metric import diversity, get_topic_coherence\n",
    "from models.atten_model import MODEL_ATT_COMP\n",
    "import swifter\n",
    "from src.models.utils import sinkhorn_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2c9dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(\"/home/ec2-user/SageMaker/github/aspect_topic_modeling/src/data/train.txt\",header = None)\n",
    "D.iloc[:,0] = D.iloc[:,0].astype(str)\n",
    "sentences = [item.split() for item in D.iloc[:,0]]\n",
    "#generate word 2 vec models\n",
    "w2vmodel = Word2Vec(sentences,vector_size=200, window=10, negative = 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77367c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input for the model\n",
    "vocab = list(set([j for i in D.values for j in i[0].split(' ') if j in w2vmodel.wv]))\n",
    "vocab = [''] + vocab\n",
    "word_track = {i: ind for ind, i in enumerate(vocab)}\n",
    "index_track = {ind: i for ind, i in enumerate(vocab)}\n",
    "#pad the input\n",
    "\n",
    "vocab_tensor = torch.Tensor([[0] * 200]  + [w2vmodel.wv[i] for i in vocab[1:]])\n",
    "vocab_ind = [torch.LongTensor([word_track[it] for it in i if it in word_track][:16]) for i in sentences]\n",
    "input = torch.nn.utils.rnn.pad_sequence(vocab_ind, batch_first=True, padding_value=0)\n",
    "\n",
    "#preprocessing to calculate the coherehence\n",
    "mlb = MultiLabelBinarizer()\n",
    "XX = mlb.fit_transform([[word_track[it] for it in i if it in word_track]  for i in sentences] + [[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ae555a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d07cee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class MODEL_ATT_COMP(nn.Module):\n",
    "    def __init__(self, d_word, d_key, d_value,n_topic, embeddings):\n",
    "        super(MODEL_ATT_COMP, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(embeddings), 200)\n",
    "        self.embeddings.weight = torch.nn.Parameter(embeddings)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.K = nn.Linear(d_word,d_key)\n",
    "        self.Q = nn.Linear(d_word,d_key)\n",
    "        self.V = nn.Linear(d_word,d_value)\n",
    "        self.V2T = nn.Linear(d_value,n_topic)\n",
    "        self.soft1 = nn.Softmax(dim = 2)\n",
    "        self.T2V = nn.Linear(n_topic,d_value)\n",
    "        self.V2W = nn.Linear(d_value,d_word)\n",
    "        self.sqrtdk = torch.tensor([d_key**0.5]).to(device)\n",
    "    \n",
    "    def loss_max_margin_neg_sample(self, x):\n",
    "        \"\"\"Maximize word level embedding reconstruction with negative sampling\n",
    "        \"\"\"\n",
    "        word_repre_x = normalize(self.word_repre, dim = 2) #batch n dvalue\n",
    "        value_recon_x = normalize(self.value_recon, dim = 2) #batch n dvalue\n",
    "        sim_matrix = torch.matmul(word_repre_x, value_recon_x.transpose(2,1)) #batch n n\n",
    "        sim_x = torch.diagonal(sim_matrix, 0, 1, 2) #batch n \n",
    "        ns = torch.randperm(sim_x.shape[1]) # n \n",
    "        loss =  1 - sim_x + torch.diagonal(sim_matrix[:, ns], 0, 1, 2)\n",
    "        loss = loss.mean(1)  #batch \n",
    "        return loss\n",
    "\n",
    "    def loss_word_prediction_no_self(self, x):\n",
    "        \"\"\"Maximize word distribution reconstruction without given word\n",
    "        \"\"\"\n",
    "        word_recon_no_self_normalized = normalize(self.word_recon_no_self, dim = 2) #batch n d_word\n",
    "        x_normalized = normalize(x, dim = 2).transpose(2,1) #batch d_word n \n",
    "        sim_matrix = torch.matmul(word_recon_no_self_normalized, x_normalized) #batch n n\n",
    "        return 1 - torch.diagonal(sim_matrix, 0, 1, 2).mean(1) #batch \n",
    "\n",
    "    def reconstruction_loss(self):\n",
    "        \"\"\"Sparsity/Entropy loss to make sure each word goes to 1 topic\n",
    "        \"\"\"\n",
    "        distribution = self.topic_weight\n",
    "        return - torch.log(distribution) * distribution\n",
    "    \n",
    "    def sinkhorn_distance(self, lambda_sh = 1):\n",
    "        \"\"\"Make sure sentence goes to less topics and documents are uniformly distributed to each topics\n",
    "        \"\"\"\n",
    "        sentence_topic = self.topic_weight.mean(1)\n",
    "        a = torch.ones(sentence_topic.shape[0]).to(device) * sentence_topic.shape[1] / sentence_topic.shape[0] #batch dimension\n",
    "        b = torch.ones(sentence_topic.shape[1]).to(device)  #topics dimension \n",
    "        #print(a.shape, b.shape)\n",
    "        return sinkhorn_torch( - torch.log(sentence_topic), a, b, lambda_sh).sum()\n",
    "    \n",
    "    \n",
    "    def similarity_loss(self):\n",
    "        \"\"\"If word has high attention on another word, they should have similar topics distribution\n",
    "        \"\"\"\n",
    "        d1, d2, d3 = self.att_weight.shape\n",
    "        normal_weights = self.att_weight.reshape(-1, d3) # batch * n n\n",
    "        samples = torch.multinomial(normal_weights, 1).reshape(-1) #batch * n\n",
    "        normalize_weights = normalize(self.topic_weight, dim = 2)\n",
    "        topic_similarity = torch.matmul(normalize_weights, normalize_weights.transpose(1,2)).reshape(d1*d2, -1) #batch n n\n",
    "        #print(topic_similarity.shape, samples.shape)\n",
    "        return 1 - topic_similarity[torch.arange(topic_similarity.shape[0]), samples].reshape(d1, d2).mean(1) #batch n\n",
    "         \n",
    "    def word_topics(self):\n",
    "        x = self.embeddings.weight\n",
    "        self.soft2 = nn.Softmax(dim = 1)\n",
    "        self.k = self.K(x).transpose(0,1) #d_key n \n",
    "        self.q = self.Q(x) #n d_key\n",
    "        self.att_score = torch.matmul(self.q, self.k) #n n\n",
    "        self.att_weight = self.soft2(self.att_score/self.sqrtdk) #n n, row sum = 1\n",
    "        self.v = self.V(x) #n d_key\n",
    "        self.word_repre = torch.matmul(self.att_weight, self.v) #batch n d_value\n",
    "        self.topic_score = self.V2T(self.word_repre) #n n_topic\n",
    "        self.word2topic = self.soft2(self.topic_score) #n n_topic, row sum = 1       \n",
    "        return self.word2topic\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: tensor, batch by n\n",
    "        Output: a dictionary that contains different loss\n",
    "        '''\n",
    "        x = self.embeddings(x) #batch n d_word\n",
    "        self.k = self.K(x).transpose(2,1) #batch d_key n \n",
    "        self.q = self.Q(x) #batch n d_key\n",
    "        self.att_score = torch.matmul(self.q, self.k) #batch n n\n",
    "        self.att_weight = self.soft1(self.att_score/self.sqrtdk) #batch n n, row sum = 1\n",
    "        self.v = self.V(x) #batch n d_key\n",
    "        self.word_repre = torch.matmul(self.att_weight, self.v) #batch n d_value\n",
    "        self.topic_score = self.V2T(self.word_repre) #batch n n_topic\n",
    "        self.topic_weight = self.soft1(self.topic_score) #batch n n_topic, row sum = 1\n",
    "        self.value_recon = self.T2V(self.topic_weight) #batch n d_value\n",
    "        self.word_recon = self.V2W(self.word_repre)#batch n d_word\n",
    "        #no self computation, effectively masked\n",
    "        #print(self.k.shape, self.att_score.shape, self.att_weight.shape, self.word_repre.shape, self.topic_weight.shape)\n",
    "        self.att_score_no_self = self.att_score -  torch.diag(torch.zeros(self.att_score.shape[1])+torch.tensor(float('inf'))).to(device)#batch n n\n",
    "        self.att_weight_no_self = self.soft1(self.att_score_no_self/self.sqrtdk) #batch n n \n",
    "        self.word_repre_no_self = torch.matmul(self.att_weight_no_self, self.v)#batch n d_key\n",
    "        self.word_recon_no_self = self.V2W(self.word_repre_no_self) #batch n d_word\n",
    "        word_pred_loss = self.loss_word_prediction_no_self(x).sum()\n",
    "        margin_loss = self.loss_max_margin_neg_sample(x).sum()\n",
    "        recon_loss = self.reconstruction_loss().mean(1).sum()\n",
    "        sim_loss = self.similarity_loss().sum()\n",
    "        sinkhorn_loss = self.sinkhorn_distance()\n",
    "        return {\n",
    "            'loss' : word_pred_loss + margin_loss,\n",
    "            'margin_loss': margin_loss,\n",
    "            'word_loss': word_pred_loss,\n",
    "            'reconstruct_loss': recon_loss,\n",
    "            'similarity_loss': sim_loss,\n",
    "            'sinkhorn_loss': sinkhorn_loss\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdc82e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(512.5452, device='cuda:0', grad_fn=<AddBackward0>) tensor(256.0012, device='cuda:0', grad_fn=<SumBackward0>) tensor(256.5441, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(500.7324, device='cuda:0', grad_fn=<AddBackward0>) tensor(255.9879, device='cuda:0', grad_fn=<SumBackward0>) tensor(244.7445, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(489.3436, device='cuda:0', grad_fn=<AddBackward0>) tensor(255.7318, device='cuda:0', grad_fn=<SumBackward0>) tensor(233.6118, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(477.1152, device='cuda:0', grad_fn=<AddBackward0>) tensor(254.1032, device='cuda:0', grad_fn=<SumBackward0>) tensor(223.0120, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(469.0170, device='cuda:0', grad_fn=<AddBackward0>) tensor(248.2777, device='cuda:0', grad_fn=<SumBackward0>) tensor(220.7392, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(448.3719, device='cuda:0', grad_fn=<AddBackward0>) tensor(232.0939, device='cuda:0', grad_fn=<SumBackward0>) tensor(216.2780, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(430.4461, device='cuda:0', grad_fn=<AddBackward0>) tensor(211.7274, device='cuda:0', grad_fn=<SumBackward0>) tensor(218.7187, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(405.1888, device='cuda:0', grad_fn=<AddBackward0>) tensor(187.7581, device='cuda:0', grad_fn=<SumBackward0>) tensor(217.4306, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(390.3582, device='cuda:0', grad_fn=<AddBackward0>) tensor(172.6524, device='cuda:0', grad_fn=<SumBackward0>) tensor(217.7058, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(367.1721, device='cuda:0', grad_fn=<AddBackward0>) tensor(150.3628, device='cuda:0', grad_fn=<SumBackward0>) tensor(216.8093, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(392.6612, device='cuda:0', grad_fn=<AddBackward0>) tensor(172.5576, device='cuda:0', grad_fn=<SumBackward0>) tensor(220.1036, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(370.2371, device='cuda:0', grad_fn=<AddBackward0>) tensor(153.1510, device='cuda:0', grad_fn=<SumBackward0>) tensor(217.0861, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(373.4702, device='cuda:0', grad_fn=<AddBackward0>) tensor(154.6689, device='cuda:0', grad_fn=<SumBackward0>) tensor(218.8014, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(321.7939, device='cuda:0', grad_fn=<AddBackward0>) tensor(104.4475, device='cuda:0', grad_fn=<SumBackward0>) tensor(217.3465, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(357.0547, device='cuda:0', grad_fn=<AddBackward0>) tensor(138.0564, device='cuda:0', grad_fn=<SumBackward0>) tensor(218.9983, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(338.3087, device='cuda:0', grad_fn=<AddBackward0>) tensor(119.6638, device='cuda:0', grad_fn=<SumBackward0>) tensor(218.6449, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(327.3091, device='cuda:0', grad_fn=<AddBackward0>) tensor(111.3363, device='cuda:0', grad_fn=<SumBackward0>) tensor(215.9728, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(325.8567, device='cuda:0', grad_fn=<AddBackward0>) tensor(110.1061, device='cuda:0', grad_fn=<SumBackward0>) tensor(215.7506, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(299.3318, device='cuda:0', grad_fn=<AddBackward0>) tensor(80.8945, device='cuda:0', grad_fn=<SumBackward0>) tensor(218.4374, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(318.0125, device='cuda:0', grad_fn=<AddBackward0>) tensor(103.5494, device='cuda:0', grad_fn=<SumBackward0>) tensor(214.4631, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------\n",
    "#model param\n",
    "#--------------------------------------------\n",
    "d_word = w2vmodel.vector_size\n",
    "n_topic = 14\n",
    "batch_size = 256\n",
    "d_key = 50\n",
    "d_value = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "#train model\n",
    "#---------------------------------------\n",
    "np.random.seed(10)\n",
    "model = MODEL_ATT_COMP(d_key = d_key, d_word = d_word, n_topic = n_topic, d_value = d_value, embeddings = vocab_tensor)\n",
    "model.to(device)\n",
    "learning_rate = 5 * 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for i in range(2000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(d['loss'], d['margin_loss'], d['word_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b6ce9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8, -0.388048617555719)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#report results for coherence and diversity\n",
    "#del emb\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a30222a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(355.1814, device='cuda:0', grad_fn=<AddBackward0>) tensor(142.3269, device='cuda:0', grad_fn=<SumBackward0>) tensor(212.8545, device='cuda:0', grad_fn=<SumBackward0>) tensor(23.7908, device='cuda:0', grad_fn=<SumBackward0>) tensor(36.8342, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(327.6125, device='cuda:0', grad_fn=<AddBackward0>) tensor(110.2729, device='cuda:0', grad_fn=<SumBackward0>) tensor(217.3396, device='cuda:0', grad_fn=<SumBackward0>) tensor(21.0638, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1769, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(325.3873, device='cuda:0', grad_fn=<AddBackward0>) tensor(114.8704, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.5169, device='cuda:0', grad_fn=<SumBackward0>) tensor(24.5505, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.7420, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(337.7707, device='cuda:0', grad_fn=<AddBackward0>) tensor(123.0372, device='cuda:0', grad_fn=<SumBackward0>) tensor(214.7335, device='cuda:0', grad_fn=<SumBackward0>) tensor(24.4918, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.9672, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(329.7029, device='cuda:0', grad_fn=<AddBackward0>) tensor(113.8797, device='cuda:0', grad_fn=<SumBackward0>) tensor(215.8232, device='cuda:0', grad_fn=<SumBackward0>) tensor(26.4798, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.6579, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(315.3743, device='cuda:0', grad_fn=<AddBackward0>) tensor(103.8337, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.5406, device='cuda:0', grad_fn=<SumBackward0>) tensor(25.2913, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.3023, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(342.1074, device='cuda:0', grad_fn=<AddBackward0>) tensor(130.5617, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.5456, device='cuda:0', grad_fn=<SumBackward0>) tensor(25.7129, device='cuda:0', grad_fn=<SumBackward0>) tensor(33.7779, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(308.7446, device='cuda:0', grad_fn=<AddBackward0>) tensor(94.6139, device='cuda:0', grad_fn=<SumBackward0>) tensor(214.1308, device='cuda:0', grad_fn=<SumBackward0>) tensor(30.9219, device='cuda:0', grad_fn=<SumBackward0>) tensor(33.7780, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(296.6740, device='cuda:0', grad_fn=<AddBackward0>) tensor(85.7028, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.9713, device='cuda:0', grad_fn=<SumBackward0>) tensor(27.4508, device='cuda:0', grad_fn=<SumBackward0>) tensor(33.6824, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(330.5558, device='cuda:0', grad_fn=<AddBackward0>) tensor(120.2588, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.2970, device='cuda:0', grad_fn=<SumBackward0>) tensor(29.7223, device='cuda:0', grad_fn=<SumBackward0>) tensor(33.8116, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(316.1895, device='cuda:0', grad_fn=<AddBackward0>) tensor(106.5955, device='cuda:0', grad_fn=<SumBackward0>) tensor(209.5939, device='cuda:0', grad_fn=<SumBackward0>) tensor(28.9629, device='cuda:0', grad_fn=<SumBackward0>) tensor(32.9765, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(300.0311, device='cuda:0', grad_fn=<AddBackward0>) tensor(86.6719, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.3592, device='cuda:0', grad_fn=<SumBackward0>) tensor(39.2221, device='cuda:0', grad_fn=<SumBackward0>) tensor(32.4503, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(335.0118, device='cuda:0', grad_fn=<AddBackward0>) tensor(120.5559, device='cuda:0', grad_fn=<SumBackward0>) tensor(214.4559, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.9862, device='cuda:0', grad_fn=<SumBackward0>) tensor(31.5959, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(356.2748, device='cuda:0', grad_fn=<AddBackward0>) tensor(143.2731, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.0017, device='cuda:0', grad_fn=<SumBackward0>) tensor(34.6615, device='cuda:0', grad_fn=<SumBackward0>) tensor(30.6507, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(339.3230, device='cuda:0', grad_fn=<AddBackward0>) tensor(126.4979, device='cuda:0', grad_fn=<SumBackward0>) tensor(212.8251, device='cuda:0', grad_fn=<SumBackward0>) tensor(39.5257, device='cuda:0', grad_fn=<SumBackward0>) tensor(30.5796, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(302.8510, device='cuda:0', grad_fn=<AddBackward0>) tensor(91.4019, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.4491, device='cuda:0', grad_fn=<SumBackward0>) tensor(40.4835, device='cuda:0', grad_fn=<SumBackward0>) tensor(29.1785, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(329.9208, device='cuda:0', grad_fn=<AddBackward0>) tensor(114.9279, device='cuda:0', grad_fn=<SumBackward0>) tensor(214.9929, device='cuda:0', grad_fn=<SumBackward0>) tensor(42.8455, device='cuda:0', grad_fn=<SumBackward0>) tensor(28.8331, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(324.6865, device='cuda:0', grad_fn=<AddBackward0>) tensor(112.1928, device='cuda:0', grad_fn=<SumBackward0>) tensor(212.4937, device='cuda:0', grad_fn=<SumBackward0>) tensor(50.2286, device='cuda:0', grad_fn=<SumBackward0>) tensor(27.1914, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(300.5870, device='cuda:0', grad_fn=<AddBackward0>) tensor(88.9374, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.6496, device='cuda:0', grad_fn=<SumBackward0>) tensor(43.4740, device='cuda:0', grad_fn=<SumBackward0>) tensor(26.0566, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(339.6805, device='cuda:0', grad_fn=<AddBackward0>) tensor(126.4322, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.2482, device='cuda:0', grad_fn=<SumBackward0>) tensor(55.5560, device='cuda:0', grad_fn=<SumBackward0>) tensor(26.0038, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss'] + 10 * d['sinkhorn_loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(d['loss'], d['margin_loss'], d['word_loss'], d['similarity_loss'], d['sinkhorn_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "581f6e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7357142857142857, -0.3700205018215475)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del emb\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "21f232c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(360.7344, device='cuda:0', grad_fn=<AddBackward0>) tensor(146.9851, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.7492, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6848, device='cuda:0', grad_fn=<SumBackward0>) tensor(36.9289, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(353.1934, device='cuda:0', grad_fn=<AddBackward0>) tensor(141.2190, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.9745, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.8667, device='cuda:0', grad_fn=<SumBackward0>) tensor(36.0786, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(307.4585, device='cuda:0', grad_fn=<AddBackward0>) tensor(91.4383, device='cuda:0', grad_fn=<SumBackward0>) tensor(216.0202, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4207, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.5880, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(294.3574, device='cuda:0', grad_fn=<AddBackward0>) tensor(81.3318, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.0256, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.2082, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3879, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(319.9689, device='cuda:0', grad_fn=<AddBackward0>) tensor(108.5279, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.4409, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4376, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(326.1772, device='cuda:0', grad_fn=<AddBackward0>) tensor(117.0180, device='cuda:0', grad_fn=<SumBackward0>) tensor(209.1591, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1504, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.2950, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(299.5901, device='cuda:0', grad_fn=<AddBackward0>) tensor(88.9316, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.6584, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.1963, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4785, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(247.1820, device='cuda:0', grad_fn=<AddBackward0>) tensor(38.4814, device='cuda:0', grad_fn=<SumBackward0>) tensor(208.7006, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.5295, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4621, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(308.5125, device='cuda:0', grad_fn=<AddBackward0>) tensor(98.2162, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.2962, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4762, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1859, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(284.8381, device='cuda:0', grad_fn=<AddBackward0>) tensor(73.4688, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.3693, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.2186, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1139, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(285.8109, device='cuda:0', grad_fn=<AddBackward0>) tensor(74.0412, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.7697, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.7056, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4088, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(278.3634, device='cuda:0', grad_fn=<AddBackward0>) tensor(68.2174, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.1460, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.9245, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3359, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(277.9633, device='cuda:0', grad_fn=<AddBackward0>) tensor(63.9970, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.9663, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.4677, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3071, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(286.1245, device='cuda:0', grad_fn=<AddBackward0>) tensor(72.9893, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.1351, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7173, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3351, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(306.1317, device='cuda:0', grad_fn=<AddBackward0>) tensor(90.5723, device='cuda:0', grad_fn=<SumBackward0>) tensor(215.5594, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1397, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3830, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(331.4871, device='cuda:0', grad_fn=<AddBackward0>) tensor(117.9373, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.5497, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7067, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1943, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(331.2661, device='cuda:0', grad_fn=<AddBackward0>) tensor(121.6315, device='cuda:0', grad_fn=<SumBackward0>) tensor(209.6346, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.6484, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.0923, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(298.0431, device='cuda:0', grad_fn=<AddBackward0>) tensor(85.4743, device='cuda:0', grad_fn=<SumBackward0>) tensor(212.5688, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.3759, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3229, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(326.9720, device='cuda:0', grad_fn=<AddBackward0>) tensor(115.8871, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.0849, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.3084, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4623, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(292.2491, device='cuda:0', grad_fn=<AddBackward0>) tensor(81.8387, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.4104, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.8400, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.0999, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss'] + d['similarity_loss'] + 10 * d['sinkhorn_loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(d['loss'], d['margin_loss'], d['word_loss'], d['similarity_loss'], d['sinkhorn_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del emb\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.topic_weight[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8b29e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
