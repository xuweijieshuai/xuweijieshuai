{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e2f1c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "#change to any directory you have to store the repo\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "\n",
    "from src.features.metric import diversity, get_topic_coherence\n",
    "from models.atten_model import MODEL_ATT_COMP\n",
    "import swifter\n",
    "from src.models.utils import sinkhorn_torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9312a679",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv(\"/home/ec2-user/SageMaker/github/aspect_topic_modeling/src/data/train.txt\",header = None)\n",
    "D.iloc[:,0] = D.iloc[:,0].astype(str)\n",
    "sentences = [item.split() for item in D.iloc[:,0]]\n",
    "#generate word 2 vec models\n",
    "w2vmodel = Word2Vec(sentences,vector_size=200, window=10, negative = 5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cab56d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate input for the model\n",
    "vocab = list(set([j for i in D.values for j in i[0].split(' ') if j in w2vmodel.wv]))\n",
    "vocab = [''] + vocab\n",
    "word_track = {i: ind for ind, i in enumerate(vocab)}\n",
    "index_track = {ind: i for ind, i in enumerate(vocab)}\n",
    "#pad the input\n",
    "\n",
    "vocab_tensor = torch.Tensor([[0] * 200]  + [w2vmodel.wv[i] for i in vocab[1:]])\n",
    "vocab_ind = [torch.LongTensor([word_track[it] for it in i if it in word_track][:16]) for i in sentences]\n",
    "input = torch.nn.utils.rnn.pad_sequence(vocab_ind, batch_first=True, padding_value=0)\n",
    "\n",
    "#preprocessing to calculate the coherehence\n",
    "mlb = MultiLabelBinarizer()\n",
    "XX = mlb.fit_transform([[word_track[it] for it in i if it in word_track]  for i in sentences] + [[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fca48771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'roll',\n",
       " 'tiny',\n",
       " 'order',\n",
       " 'anyway',\n",
       " 'often',\n",
       " 'get',\n",
       " 'order',\n",
       " 'wrong',\n",
       " 'stray',\n",
       " 'menu']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4c9006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import normalize\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class MODEL_ATT_COMP(nn.Module):\n",
    "    def __init__(self, d_word, d_key, d_value, n_topic, n, embeddings):\n",
    "        super(MODEL_ATT_COMP, self).__init__()\n",
    "        self.embeddings = nn.Embedding(len(embeddings), 200)\n",
    "        self.embeddings.weight = torch.nn.Parameter(embeddings)\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.K = nn.Linear(d_word,d_key)\n",
    "        self.Q = nn.Linear(d_word,d_key)\n",
    "        self.V = nn.Linear(d_word,d_value)\n",
    "        self.V2T = nn.Linear(d_value,n_topic)\n",
    "        self.T2L = nn.Linear(n, 1)\n",
    "        self.L2T = nn.Linear(1, n)\n",
    "        self.soft1 = nn.Softmax(dim = 2)\n",
    "        self.T2V = nn.Linear(n_topic,d_value)\n",
    "        self.V2W = nn.Linear(d_value,d_word)\n",
    "        self.sqrtdk = torch.tensor([d_key**0.5]).to(device)\n",
    "        self.soft2 = nn.Softmax(dim = 1)\n",
    "    \n",
    "    def loss_max_margin_neg_sample(self, x):\n",
    "        \"\"\"Maximize word level embedding reconstruction with negative sampling\n",
    "        \"\"\"\n",
    "        word_repre_x = normalize(self.word_repre, dim = 2) #batch n dvalue\n",
    "        value_recon_x = normalize(self.value_recon, dim = 2) #batch n dvalue\n",
    "        sim_matrix = torch.matmul(word_repre_x, value_recon_x.transpose(2,1)) #batch n n\n",
    "        sim_x = torch.diagonal(sim_matrix, 0, 1, 2) #batch n \n",
    "        ns = torch.randperm(sim_x.shape[1]) # n \n",
    "        loss =  1 - sim_x + torch.diagonal(sim_matrix[:, ns], 0, 1, 2)\n",
    "        loss = loss.mean(1)  #batch \n",
    "        return loss\n",
    "\n",
    "    def loss_word_prediction_no_self(self, x):\n",
    "        \"\"\"Maximize word distribution reconstruction without given word\n",
    "        \"\"\"\n",
    "        word_recon_no_self_normalized = normalize(self.word_recon_no_self, dim = 2) #batch n d_word\n",
    "        x_normalized = normalize(x, dim = 2).transpose(2,1) #batch d_word n \n",
    "        sim_matrix = torch.matmul(word_recon_no_self_normalized, x_normalized) #batch n n\n",
    "        return 1 - torch.diagonal(sim_matrix, 0, 1, 2).mean(1) #batch \n",
    "\n",
    "    def reconstruction_loss(self):\n",
    "        \"\"\"Sparsity/Entropy loss to make sure each word goes to 1 topic\n",
    "        \"\"\"\n",
    "        distribution = self.topic\n",
    "        return - torch.log(distribution) * distribution\n",
    "    \n",
    "    def sinkhorn_distance(self, lambda_sh = 10):\n",
    "        \"\"\"Make sure sentence goes to less topics and documents are uniformly distributed to each topics\n",
    "        \"\"\"\n",
    "        d1, d2, d3 = self.topic.shape\n",
    "        sentence_topic = self.topic.reshape(d1, d2)\n",
    "        a = torch.ones(sentence_topic.shape[0]).to(device) * sentence_topic.shape[1] / sentence_topic.shape[0] #batch dimension\n",
    "        b = torch.ones(sentence_topic.shape[1]).to(device)  #topics dimension \n",
    "        #print(a.shape, b.shape)\n",
    "        return sinkhorn_torch( - torch.log(sentence_topic + 1e-6), a, b, lambda_sh).sum()\n",
    "    \n",
    "    \n",
    "    def similarity_loss(self):\n",
    "        \"\"\"If word has high attention on another word, they should have similar topics distribution\n",
    "        \"\"\"\n",
    "        d1, d2, d3 = self.att_weight.shape\n",
    "        normal_weights = self.att_weight.reshape(-1, d3) # batch * n n\n",
    "        #print(normal_weights.shape)\n",
    "        samples = torch.multinomial(normal_weights, 1).reshape(-1) #batch * n\n",
    "        normalize_weights = normalize(self.topic_weight, dim = 2)\n",
    "        #print(normalize_weights.shape)\n",
    "        topic_similarity = torch.matmul(normalize_weights, normalize_weights.transpose(1,2)).reshape(d1*d2, -1) #batch n n\n",
    "        #print(topic_similarity.shape, samples.shape)\n",
    "        return 1 - topic_similarity[torch.arange(topic_similarity.shape[0]), samples].reshape(d1, d2).mean(1) #batch n\n",
    "         \n",
    "    def word_topics(self):\n",
    "        x = self.embeddings.weight\n",
    "        self.soft2 = nn.Softmax(dim = 1)\n",
    "        self.k = self.K(x).transpose(0,1) #d_key n \n",
    "        self.q = self.Q(x) #n d_key\n",
    "        self.att_score = torch.matmul(self.q, self.k) #n n\n",
    "        self.att_weight = self.soft2(self.att_score/self.sqrtdk) #n n, row sum = 1\n",
    "        self.v = self.V(x) #n d_key\n",
    "        self.word_repre = torch.matmul(self.att_weight, self.v) #batch n d_value\n",
    "        self.topic_score = self.V2T(self.word_repre) #n n_topic\n",
    "        self.word2topic = self.soft2(self.topic_score) #n n_topic, row sum = 1       \n",
    "        return self.word2topic\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: tensor, batch by n\n",
    "        Output: a dictionary that contains different loss\n",
    "        '''\n",
    "        x = self.embeddings(x) #batch n d_word\n",
    "        self.k = self.K(x).transpose(2,1) #batch d_key n \n",
    "        self.q = self.Q(x) #batch n d_key\n",
    "        self.att_score = torch.matmul(self.q, self.k) #batch n n\n",
    "        self.att_weight = self.soft1(self.att_score/self.sqrtdk) #batch n n, row sum = 1\n",
    "        self.v = self.V(x) #batch n d_key\n",
    "        self.word_repre = torch.matmul(self.att_weight, self.v) #batch n d_value\n",
    "        self.topic_score = self.V2T(self.word_repre) #batch n n_topic\n",
    "        self.topic_weight = self.soft1(self.topic_score) #batch n n_topic , row sum = 1\n",
    "        #print(self.topic_weight.shape)\n",
    "        self.topic = self.soft2(self.T2L(self.topic_weight.transpose(2,1))) #batch n_topic 1\n",
    "        #print(self.topic.shape)\n",
    "        self.topic_recon = self.L2T(self.topic).transpose(2,1) #batch  n n_topic\n",
    "        #print(self.topic_recon.shape)\n",
    "        self.value_recon = self.T2V(self.topic_recon) #batch n d_value\n",
    "        self.word_recon = self.V2W(self.word_repre)#batch n d_word\n",
    "        #no self computation, effectively masked\n",
    "        #print(self.k.shape, self.att_score.shape, self.att_weight.shape, self.word_repre.shape, self.topic_weight.shape)\n",
    "        self.att_score_no_self = self.att_score -  torch.diag(torch.zeros(self.att_score.shape[1])+torch.tensor(float('inf'))).to(device)#batch n n\n",
    "        self.att_weight_no_self = self.soft1(self.att_score_no_self/self.sqrtdk) #batch n n \n",
    "        self.word_repre_no_self = torch.matmul(self.att_weight_no_self, self.v)#batch n d_key\n",
    "        self.word_recon_no_self = self.V2W(self.word_repre_no_self) #batch n d_word\n",
    "        word_pred_loss = self.loss_word_prediction_no_self(x).sum()\n",
    "        margin_loss = self.loss_max_margin_neg_sample(x).sum()\n",
    "        recon_loss = self.reconstruction_loss().sum()\n",
    "        sim_loss = self.similarity_loss().sum()\n",
    "        sinkhorn_loss = self.sinkhorn_distance()\n",
    "        return {\n",
    "            'loss' : word_pred_loss + margin_loss,\n",
    "            'margin_loss': margin_loss,\n",
    "            'word_loss': word_pred_loss,\n",
    "            'reconstruct_loss': recon_loss,\n",
    "            'similarity_loss': sim_loss,\n",
    "            'sinkhorn_loss': sinkhorn_loss\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d314fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5602, device='cuda:0', grad_fn=<DivBackward0>) tensor(2.5661, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.3695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(255.9141, device='cuda:0', grad_fn=<DivBackward0>) tensor(251.8075, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0181, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(255.7092, device='cuda:0', grad_fn=<DivBackward0>) tensor(240.7245, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0865, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(255.1251, device='cuda:0', grad_fn=<DivBackward0>) tensor(230.5776, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1839, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(254.6742, device='cuda:0', grad_fn=<DivBackward0>) tensor(223.4379, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2209, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(254.0700, device='cuda:0', grad_fn=<DivBackward0>) tensor(218.9606, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.2187, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(253.7453, device='cuda:0', grad_fn=<DivBackward0>) tensor(215.8251, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1991, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9457, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(252.1936, device='cuda:0', grad_fn=<DivBackward0>) tensor(213.6863, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1773, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(251.4662, device='cuda:0', grad_fn=<DivBackward0>) tensor(212.2623, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1510, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(250.4513, device='cuda:0', grad_fn=<DivBackward0>) tensor(210.8972, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1293, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(249.2223, device='cuda:0', grad_fn=<DivBackward0>) tensor(209.1665, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1078, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(246.2200, device='cuda:0', grad_fn=<DivBackward0>) tensor(208.6441, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0884, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(248.2029, device='cuda:0', grad_fn=<DivBackward0>) tensor(207.6485, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0735, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9464, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(244.1939, device='cuda:0', grad_fn=<DivBackward0>) tensor(207.1192, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0618, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(242.1789, device='cuda:0', grad_fn=<DivBackward0>) tensor(206.5322, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0509, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(240.0495, device='cuda:0', grad_fn=<DivBackward0>) tensor(206.4971, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0427, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(241.4507, device='cuda:0', grad_fn=<DivBackward0>) tensor(205.8804, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0364, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9465, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(240.2302, device='cuda:0', grad_fn=<DivBackward0>) tensor(205.4728, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0316, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(236.1067, device='cuda:0', grad_fn=<DivBackward0>) tensor(205.2674, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0265, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(235.8148, device='cuda:0', grad_fn=<DivBackward0>) tensor(204.9363, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0230, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(239.2606, device='cuda:0', grad_fn=<DivBackward0>) tensor(204.4649, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0201, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(237.8722, device='cuda:0', grad_fn=<DivBackward0>) tensor(204.2503, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0178, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(230.6951, device='cuda:0', grad_fn=<DivBackward0>) tensor(203.8632, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0158, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(231.5104, device='cuda:0', grad_fn=<DivBackward0>) tensor(203.7613, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0137, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(228.9835, device='cuda:0', grad_fn=<DivBackward0>) tensor(203.2869, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0117, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(228.2986, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.9247, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0104, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(232.8520, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.7857, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0091, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(227.5031, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.7848, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0081, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(228.1267, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.6379, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0073, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(222.9281, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.4858, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0066, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(230.1874, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.2669, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0059, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(223.9454, device='cuda:0', grad_fn=<DivBackward0>) tensor(202.3704, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0053, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(230.7373, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.9249, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0049, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(222.4440, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.8953, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0045, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(225.3016, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.5569, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0041, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(224.8226, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.0237, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0037, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(222.1388, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.1265, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0034, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(225.7466, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.5566, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0031, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(220.2372, device='cuda:0', grad_fn=<DivBackward0>) tensor(201.2594, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0028, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9468, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(217.3984, device='cuda:0', grad_fn=<DivBackward0>) tensor(200.8024, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0026, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9468, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------\n",
    "#model param\n",
    "#--------------------------------------------\n",
    "d_word = w2vmodel.vector_size\n",
    "n_topic = 14\n",
    "batch_size = 256\n",
    "d_key = 50\n",
    "d_value = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "#---------------------------------------\n",
    "#train model\n",
    "#---------------------------------------\n",
    "np.random.seed(10)\n",
    "model = MODEL_ATT_COMP(d_key = d_key, d_word = d_word, n_topic = n_topic, d_value = d_value, n = 16, embeddings = vocab_tensor)\n",
    "model.to(device)\n",
    "learning_rate = 5 * 1e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "margin_loss, word_loss, similarity_loss, sinkhorn_loss = 0, 0, 0, 0\n",
    "for i in range(4000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss']  +  d['sinkhorn_loss'] + d['similarity_loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    margin_loss += d['margin_loss']\n",
    "    word_loss += d['word_loss']\n",
    "    similarity_loss += d['similarity_loss']\n",
    "    sinkhorn_loss += d['sinkhorn_loss']\n",
    "    if i % 100 == 0:\n",
    "        print(margin_loss/100, word_loss/100, similarity_loss/100, sinkhorn_loss/100)\n",
    "        margin_loss, word_loss, similarity_loss, sinkhorn_loss = 0, 0, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "01356208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0796, 0.0745, 0.0747, 0.0716, 0.0590, 0.0697, 0.0756, 0.0811, 0.0744,\n",
       "        0.0712, 0.0703, 0.0635, 0.0646, 0.0703], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d46d1ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['dark',\n",
       "  'lighting',\n",
       "  'banquette',\n",
       "  'window',\n",
       "  'lit',\n",
       "  'scene',\n",
       "  'brick',\n",
       "  'ceiling',\n",
       "  'wood',\n",
       "  'wall'],\n",
       " ['10', 'min', 'tasty', '20', 'arrived', '30', '15', '45', 'waited', 'yummy'],\n",
       " ['exposed',\n",
       "  'floor',\n",
       "  'window',\n",
       "  'banquette',\n",
       "  'lit',\n",
       "  'scene',\n",
       "  'wall',\n",
       "  'ceiling',\n",
       "  'wood',\n",
       "  'brick'],\n",
       " ['american',\n",
       "  'authentic',\n",
       "  'simple',\n",
       "  'cuisine',\n",
       "  'traditional',\n",
       "  'presentation',\n",
       "  'fare',\n",
       "  'combination',\n",
       "  'flavor',\n",
       "  'ingredient'],\n",
       " ['cocktail',\n",
       "  'dessert',\n",
       "  'sangria',\n",
       "  'course',\n",
       "  'appetizer',\n",
       "  'wine',\n",
       "  'fixe',\n",
       "  'prix',\n",
       "  'selection',\n",
       "  'entree'],\n",
       " ['chair',\n",
       "  'floor',\n",
       "  'banquette',\n",
       "  'window',\n",
       "  'lit',\n",
       "  'scene',\n",
       "  'brick',\n",
       "  'ceiling',\n",
       "  'wood',\n",
       "  'wall'],\n",
       " ['creative',\n",
       "  'thai',\n",
       "  'fare',\n",
       "  'quality',\n",
       "  'american',\n",
       "  'fusion',\n",
       "  'asian',\n",
       "  'japanese',\n",
       "  'authentic',\n",
       "  'cuisine'],\n",
       " ['crab',\n",
       "  'tender',\n",
       "  'sweet',\n",
       "  'mashed',\n",
       "  'garlic',\n",
       "  'cake',\n",
       "  'dry',\n",
       "  'banana',\n",
       "  'cream',\n",
       "  'chocolate'],\n",
       " ['sunday',\n",
       "  'great',\n",
       "  'friendly',\n",
       "  'attentive',\n",
       "  'birthday',\n",
       "  'went',\n",
       "  'friend',\n",
       "  'dinner',\n",
       "  'fixe',\n",
       "  'prix'],\n",
       " ['tip',\n",
       "  'someone',\n",
       "  'check',\n",
       "  'credit',\n",
       "  'u',\n",
       "  'people',\n",
       "  'card',\n",
       "  'person',\n",
       "  'bill',\n",
       "  'customer'],\n",
       " ['dinner',\n",
       "  'ordering',\n",
       "  'tip',\n",
       "  'entree',\n",
       "  'week',\n",
       "  'person',\n",
       "  'order',\n",
       "  'u',\n",
       "  'drink',\n",
       "  'bill'],\n",
       " ['friendly',\n",
       "  'birthday',\n",
       "  'server',\n",
       "  'friend',\n",
       "  'went',\n",
       "  'bartender',\n",
       "  'attentive',\n",
       "  'waitstaff',\n",
       "  'staff',\n",
       "  'service'],\n",
       " ['exposed',\n",
       "  'floor',\n",
       "  'banquette',\n",
       "  'window',\n",
       "  'lit',\n",
       "  'scene',\n",
       "  'brick',\n",
       "  'ceiling',\n",
       "  'wood',\n",
       "  'wall'],\n",
       " ['chair',\n",
       "  'floor',\n",
       "  'banquette',\n",
       "  'window',\n",
       "  'lit',\n",
       "  'scene',\n",
       "  'brick',\n",
       "  'ceiling',\n",
       "  'wood',\n",
       "  'wall']]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6587e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.34285714285714286, -0.014083204172458997)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#report results for coherence and diversity\n",
    "del emb\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef0dab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4330, device='cuda:0', grad_fn=<DivBackward0>) tensor(1.9773, device='cuda:0', grad_fn=<DivBackward0>) tensor(5.5597e-05, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.3695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(175.3267, device='cuda:0', grad_fn=<DivBackward0>) tensor(200.1168, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(170.5996, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.7471, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9463, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(177.2196, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.7489, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0054, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(164.9298, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.4720, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9462, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(174.3363, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.6397, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(172.0985, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.6910, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0056, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9461, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(176.9106, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.3965, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0059, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(178.4284, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.0741, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0063, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9459, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(176.0560, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.4530, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0069, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9458, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(174.4758, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.2911, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0077, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9456, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(175.1233, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.2505, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0093, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9453, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(175.0131, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.4242, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0117, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(171.2294, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.5686, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0167, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(175.8457, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.1311, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0311, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9417, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(169.6736, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.2257, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.0606, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9376, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(177.9752, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.0119, device='cuda:0', grad_fn=<DivBackward0>) tensor(0.1855, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.9214, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(173.3181, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.4395, device='cuda:0', grad_fn=<DivBackward0>) tensor(1.1842, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.7941, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(174.2253, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.5504, device='cuda:0', grad_fn=<DivBackward0>) tensor(4.1999, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.3428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(177.0440, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.1506, device='cuda:0', grad_fn=<DivBackward0>) tensor(5.2040, device='cuda:0', grad_fn=<DivBackward0>) tensor(36.1037, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(171.3467, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.3618, device='cuda:0', grad_fn=<DivBackward0>) tensor(5.5016, device='cuda:0', grad_fn=<DivBackward0>) tensor(35.9677, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(168.3380, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.5986, device='cuda:0', grad_fn=<DivBackward0>) tensor(5.8103, device='cuda:0', grad_fn=<DivBackward0>) tensor(35.8437, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(170.2457, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.7156, device='cuda:0', grad_fn=<DivBackward0>) tensor(6.0553, device='cuda:0', grad_fn=<DivBackward0>) tensor(35.7404, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(169.1695, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.1909, device='cuda:0', grad_fn=<DivBackward0>) tensor(6.2605, device='cuda:0', grad_fn=<DivBackward0>) tensor(35.6325, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(170.3581, device='cuda:0', grad_fn=<DivBackward0>) tensor(199.1012, device='cuda:0', grad_fn=<DivBackward0>) tensor(6.4068, device='cuda:0', grad_fn=<DivBackward0>) tensor(35.5275, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-28ae81d0d807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0;36m10\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sinkhorn_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmargin_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'margin_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "margin_loss, word_loss, similarity_loss, sinkhorn_loss = 0, 0, 0, 0\n",
    "for i in range(4000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss'] +  d['sinkhorn_loss'] + d['similarity_loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    margin_loss += d['margin_loss']\n",
    "    word_loss += d['word_loss']\n",
    "    similarity_loss += d['similarity_loss']\n",
    "    sinkhorn_loss += d['sinkhorn_loss']\n",
    "    if i % 100 == 0:\n",
    "        print(margin_loss/100, word_loss/100, similarity_loss/100, sinkhorn_loss/100)\n",
    "        margin_loss, word_loss, similarity_loss, sinkhorn_loss = 0, 0, 0, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2509cea0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cb67928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0023, 0.0069, 0.0038, 0.0069, 0.0022, 0.0052, 0.9222, 0.0010, 0.0029,\n",
       "        0.0031, 0.0044, 0.0296, 0.0082, 0.0012], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b1bb18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.37857142857142856, -0.48443663506943274)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del emb\n",
    "import gc\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8e12a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.42857142857142855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15679243820582905"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "print(np.mean(diversity(topics)))\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9549cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(360.7344, device='cuda:0', grad_fn=<AddBackward0>) tensor(146.9851, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.7492, device='cuda:0', grad_fn=<SumBackward0>) tensor(0.6848, device='cuda:0', grad_fn=<SumBackward0>) tensor(36.9289, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(353.1934, device='cuda:0', grad_fn=<AddBackward0>) tensor(141.2190, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.9745, device='cuda:0', grad_fn=<SumBackward0>) tensor(3.8667, device='cuda:0', grad_fn=<SumBackward0>) tensor(36.0786, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(307.4585, device='cuda:0', grad_fn=<AddBackward0>) tensor(91.4383, device='cuda:0', grad_fn=<SumBackward0>) tensor(216.0202, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4207, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.5880, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(294.3574, device='cuda:0', grad_fn=<AddBackward0>) tensor(81.3318, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.0256, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.2082, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3879, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(319.9689, device='cuda:0', grad_fn=<AddBackward0>) tensor(108.5279, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.4409, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4376, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3275, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(326.1772, device='cuda:0', grad_fn=<AddBackward0>) tensor(117.0180, device='cuda:0', grad_fn=<SumBackward0>) tensor(209.1591, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1504, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.2950, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(299.5901, device='cuda:0', grad_fn=<AddBackward0>) tensor(88.9316, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.6584, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.1963, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4785, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(247.1820, device='cuda:0', grad_fn=<AddBackward0>) tensor(38.4814, device='cuda:0', grad_fn=<SumBackward0>) tensor(208.7006, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.5295, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4621, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(308.5125, device='cuda:0', grad_fn=<AddBackward0>) tensor(98.2162, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.2962, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.4762, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1859, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(284.8381, device='cuda:0', grad_fn=<AddBackward0>) tensor(73.4688, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.3693, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.2186, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1139, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(285.8109, device='cuda:0', grad_fn=<AddBackward0>) tensor(74.0412, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.7697, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.7056, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4088, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(278.3634, device='cuda:0', grad_fn=<AddBackward0>) tensor(68.2174, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.1460, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.9245, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3359, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(277.9633, device='cuda:0', grad_fn=<AddBackward0>) tensor(63.9970, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.9663, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.4677, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3071, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(286.1245, device='cuda:0', grad_fn=<AddBackward0>) tensor(72.9893, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.1351, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7173, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3351, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(306.1317, device='cuda:0', grad_fn=<AddBackward0>) tensor(90.5723, device='cuda:0', grad_fn=<SumBackward0>) tensor(215.5594, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.1397, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3830, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(331.4871, device='cuda:0', grad_fn=<AddBackward0>) tensor(117.9373, device='cuda:0', grad_fn=<SumBackward0>) tensor(213.5497, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.7067, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.1943, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(331.2661, device='cuda:0', grad_fn=<AddBackward0>) tensor(121.6315, device='cuda:0', grad_fn=<SumBackward0>) tensor(209.6346, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.6484, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.0923, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(298.0431, device='cuda:0', grad_fn=<AddBackward0>) tensor(85.4743, device='cuda:0', grad_fn=<SumBackward0>) tensor(212.5688, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.3759, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.3229, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(326.9720, device='cuda:0', grad_fn=<AddBackward0>) tensor(115.8871, device='cuda:0', grad_fn=<SumBackward0>) tensor(211.0849, device='cuda:0', grad_fn=<SumBackward0>) tensor(7.3084, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.4623, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(292.2491, device='cuda:0', grad_fn=<AddBackward0>) tensor(81.8387, device='cuda:0', grad_fn=<SumBackward0>) tensor(210.4104, device='cuda:0', grad_fn=<SumBackward0>) tensor(6.8400, device='cuda:0', grad_fn=<SumBackward0>) tensor(35.0999, device='cuda:0', grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2000):\n",
    "    idx_batch = np.random.choice(np.arange(D.shape[0]),batch_size, replace = False)\n",
    "    x = input[idx_batch].to(device)\n",
    "    d = model.forward(x)\n",
    "    loss = d['loss'] + d['similarity_loss'] + 10 * d['sinkhorn_loss']\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 100 == 0:\n",
    "        print(d['loss'], d['margin_loss'], d['word_loss'], d['similarity_loss'], d['sinkhorn_loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75126d8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5428571428571428, -0.03780059480179641)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del emb\n",
    "gc.collect()\n",
    "emb = model.word_topics().cpu()\n",
    "topics = [[vocab[j] for j in i] for i in emb.argsort(0)[-10:, :].t().detach().cpu().numpy() ]\n",
    "coherences= get_topic_coherence(XX, topics, word_track)\n",
    "np.mean(diversity(topics)), coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "02338213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuna', 4),\n",
       " ('mushroom', 4),\n",
       " ('roasted', 4),\n",
       " ('tomato', 4),\n",
       " ('grilled', 4),\n",
       " ('sauce', 4),\n",
       " ('business', 3),\n",
       " ('people', 3),\n",
       " ('around', 3),\n",
       " ('decided', 3),\n",
       " ('tender', 3),\n",
       " ('gras', 3),\n",
       " ('salad', 3),\n",
       " ('onion', 3),\n",
       " ('bathroom', 2),\n",
       " ('seating', 2),\n",
       " ('young', 2),\n",
       " ('movie', 2),\n",
       " ('crowd', 2),\n",
       " ('chair', 2),\n",
       " ('lounge', 2),\n",
       " ('street', 2),\n",
       " ('open', 2),\n",
       " ('see', 2),\n",
       " ('ago', 2),\n",
       " ('since', 2),\n",
       " ('creamy', 2),\n",
       " ('better', 2),\n",
       " ('cheap', 2),\n",
       " ('aged', 1),\n",
       " ('smooth', 1),\n",
       " ('oven', 1),\n",
       " ('color', 1),\n",
       " ('touch', 1),\n",
       " ('warm', 1),\n",
       " ('flower', 1),\n",
       " ('beautifully', 1),\n",
       " ('wood', 1),\n",
       " ('presentation', 1),\n",
       " ('cuisine', 1),\n",
       " ('citysearch', 1),\n",
       " ('week', 1),\n",
       " ('year', 1),\n",
       " ('advance', 1),\n",
       " ('wife', 1),\n",
       " ('wedding', 1),\n",
       " ('event', 1),\n",
       " ('celebrate', 1),\n",
       " ('greeted', 1),\n",
       " ('evening', 1),\n",
       " ('anniversary', 1),\n",
       " ('birthday', 1),\n",
       " ('reservation', 1),\n",
       " ('burnt', 1),\n",
       " ('hamburger', 1),\n",
       " ('medium', 1),\n",
       " ('plate', 1),\n",
       " ('dessert', 1),\n",
       " ('bland', 1),\n",
       " ('stick', 1),\n",
       " ('slice', 1),\n",
       " ('bbq', 1),\n",
       " ('try', 1),\n",
       " ('bass', 1),\n",
       " ('long', 1),\n",
       " ('one', 1),\n",
       " ('crowded', 1),\n",
       " ('shrimp', 1),\n",
       " ('size', 1),\n",
       " ('like', 1),\n",
       " ('great', 1),\n",
       " ('decent', 1),\n",
       " ('drink', 1),\n",
       " ('asking', 1),\n",
       " ('tip', 1),\n",
       " ('waitress', 1),\n",
       " ('manager', 1),\n",
       " ('minute', 1),\n",
       " ('waiter', 1),\n",
       " ('bill', 1),\n",
       " ('asked', 1),\n",
       " ('u', 1),\n",
       " ('outside', 1),\n",
       " ('care', 1),\n",
       " ('bad', 1),\n",
       " ('rather', 1),\n",
       " ('seem', 1),\n",
       " ('poor', 1),\n",
       " ('service', 1),\n",
       " ('friendly', 1),\n",
       " ('average', 1)]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter([j for i in topics for j in i]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e47ca36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
