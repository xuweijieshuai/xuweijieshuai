{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35a5e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "\n",
    "from src.models.utils import remove_stopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd9039",
   "metadata": {},
   "source": [
    "# 20News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd45c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 20News Data\n",
    "print('reading data...')\n",
    "train_data = fetch_20newsgroups(subset='train')\n",
    "test_data = fetch_20newsgroups(subset='test')\n",
    "len(train_data['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5337220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_punctuation(w):\n",
    "    return any(char in string.punctuation for char in w)\n",
    "\n",
    "\n",
    "\n",
    "init_docs_tr = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', train_data.data[doc]) for doc in range(len(train_data.data))]\n",
    "init_docs_ts = [re.findall(r'''[\\w']+|[.,!?;-~{}`´_<=>:/@*()&'$%#\"]''', test_data.data[doc]) for doc in range(len(test_data.data))]\n",
    "\n",
    "init_docs = init_docs_tr + init_docs_ts\n",
    "init_docs = [[w.lower() for w in init_docs[doc] if not contains_punctuation(w)] for doc in range(len(init_docs))]\n",
    "#init_docs = [[w for w in init_docs[doc] if not contains_numeric(w)] for doc in range(len(init_docs))]\n",
    "sentences = [[w for w in init_docs[doc] if len(w)>1] for doc in range(len(init_docs))]\n",
    "#init_docs = [\" \".join(init_docs[doc]) for doc in range(len(init_docs))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "419a5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate word 2 vec models\n",
    "w2vmodel = Word2Vec(init_docs,vector_size=200, window=10, negative = 5, min_count= 10)\n",
    "\n",
    "#generate input for the model\n",
    "vocab = list(set([j for i in init_docs for j in i if j in w2vmodel.wv]))\n",
    "vocab = [''] + vocab\n",
    "word_track = {i: ind for ind, i in enumerate(vocab)}\n",
    "index_track = {ind: i for ind, i in enumerate(vocab)}\n",
    "#pad the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e2f7ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile([len(i) for i in sentences], 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ab4442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab_tensor = torch.Tensor([[0] * 200]  + [w2vmodel.wv[i] for i in vocab[1:]])\n",
    "vocab_ind = [torch.LongTensor([word_track[it] for it in i if it in word_track][:384]) for i in sentences]\n",
    "input = torch.nn.utils.rnn.pad_sequence(vocab_ind, batch_first=True, padding_value=0)\n",
    "\n",
    "#preprocessing to calculate the coherehence\n",
    "mlb = MultiLabelBinarizer()\n",
    "XX = mlb.fit_transform([[word_track[it] for it in i if it in word_track]  for i in sentences] + [[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0da5ea",
   "metadata": {},
   "source": [
    "# AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82ea8c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv('https://raw.githubusercontent.com/yumeng5/WeSTClass/master/agnews/dataset.csv', names = ['class', 'documents', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ba292e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "D['clean_text'] = D['documents'] + D['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fc6aac9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d2cdfb02ab4dfa8301c2cf205fe08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D['clean_text'] = D.swifter.apply(lambda x: ' '.join(remove_stopWords(x['documents'] + x['text'])).split(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0feae04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate word 2 vec models\n",
    "w2vmodel = Word2Vec(D['clean_text'].values,vector_size=200, window=10, negative = 5, min_count= 10)\n",
    "\n",
    "#generate input for the model\n",
    "vocab = list(set([j for i in init_docs for j in i if j in w2vmodel.wv]))\n",
    "vocab = [''] + vocab\n",
    "word_track = {i: ind for ind, i in enumerate(vocab)}\n",
    "index_track = {ind: i for ind, i in enumerate(vocab)}\n",
    "#pad the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2973e184",
   "metadata": {},
   "source": [
    "# RCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecf030e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42707da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20177851",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup dataset NYT and Arxiv\n",
    "\n",
    "#https://github.com/yumeng5/JoSH/tree/master/datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
