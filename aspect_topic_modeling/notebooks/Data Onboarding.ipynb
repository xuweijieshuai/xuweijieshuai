{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "e377d9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import pickle \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn, optim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from scipy import sparse\n",
    "import itertools\n",
    "from scipy.io import savemat, loadmat\n",
    "import re\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "import swifter\n",
    "from src.models.utils import remove_stopWords\n",
    "from src.models.utils import get_wordnet_pos, remove_stopWords, get_emb, generate_emb, train, kld_normal, get_common_words, generate_bow\n",
    "from collections import Counter\n",
    "from models.NVDM import topic_covariance_penalty, sinkhorn_torch, NTM, negative_sampling_prior, optimal_transport_prior,  NormalParameter, get_mlp, EmbTopic, NSSTM, OTETM\n",
    "from src.models.utils import get_wordnet_pos, remove_stopWords, get_emb, generate_emb, train, kld_normal, get_common_words\n",
    "from hyperspherical_vae.distributions import VonMisesFisher\n",
    "from hyperspherical_vae.distributions import HypersphericalUniform\n",
    "from src.features.metric import diversity, get_topic_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9685e105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/github/aspect_topic_modeling/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e426dd1a",
   "metadata": {},
   "source": [
    "# 20News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 915,
   "id": "942110ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n"
     ]
    }
   ],
   "source": [
    "# 20News Data\n",
    "print('reading data...')\n",
    "data = fetch_20newsgroups(subset = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3c87cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "789c8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['class'] = data['target']\n",
    "df['text'] = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "157caa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18846, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "651da5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004859817b45407786070c8c09ef2e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d20b177e7c3d40d192cd355c811c063a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/18846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df.swifter.apply(lambda x: ' '.join(remove_stopWords(x['text'])), axis=1)\n",
    "df['clean_text']  = df.swifter.apply(lambda x: x['text'].split(), axis = 1)\n",
    "\n",
    "common_words_ct = Counter([j for i in df['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 200)\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)}\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)}\n",
    "df['index_num'] = df.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ac2b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/20news/text.txt',\n",
    "           df['text'].values,\n",
    "           fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "327e9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change it to any location you save your embeddings\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/20news/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = df, common_words = common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 918,
   "id": "61787c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2394"
      ]
     },
     "execution_count": 918,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "id": "6a691a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['andrew',\n",
       " 'cmu',\n",
       " 'edu',\n",
       " 'subject',\n",
       " 'pen',\n",
       " 'fan',\n",
       " 'reaction',\n",
       " 'organization',\n",
       " 'post',\n",
       " 'office',\n",
       " 'carnegie',\n",
       " 'mellon',\n",
       " 'pittsburgh',\n",
       " 'line',\n",
       " 'nntp',\n",
       " 'host',\n",
       " 'sure',\n",
       " 'pretty',\n",
       " 'confuse',\n",
       " 'lack',\n",
       " 'kind',\n",
       " 'recent',\n",
       " 'massacre',\n",
       " 'devil',\n",
       " 'actually',\n",
       " 'bit',\n",
       " 'end',\n",
       " 'non',\n",
       " 'man',\n",
       " 'kill',\n",
       " 'bad',\n",
       " 'think',\n",
       " 'good',\n",
       " 'regular',\n",
       " 'season',\n",
       " 'stats',\n",
       " 'lot',\n",
       " 'fun',\n",
       " 'watch',\n",
       " 'playoff',\n",
       " 'let',\n",
       " 'couple',\n",
       " 'game',\n",
       " 'beat',\n",
       " 'lose',\n",
       " 'final',\n",
       " 'rule',\n",
       " 'midway',\n",
       " 'ecn',\n",
       " 'uoknor',\n",
       " 'matthew',\n",
       " 'high',\n",
       " 'performance',\n",
       " 'vlb',\n",
       " 'video',\n",
       " 'card',\n",
       " 'summary',\n",
       " 'seek',\n",
       " 'engineering',\n",
       " 'computer',\n",
       " 'network',\n",
       " 'university',\n",
       " 'usa',\n",
       " 'keywords',\n",
       " 'brother',\n",
       " 'market',\n",
       " 'support',\n",
       " 'vesa',\n",
       " 'local',\n",
       " 'bus',\n",
       " 'ram',\n",
       " 'suggestion',\n",
       " 'idea',\n",
       " 'diamond',\n",
       " 'pro',\n",
       " 'ati',\n",
       " 'graphic',\n",
       " 'email',\n",
       " 'thank',\n",
       " 'matt',\n",
       " 'king',\n",
       " 'heaven',\n",
       " 'right',\n",
       " 'way',\n",
       " 'armenia',\n",
       " 'say',\n",
       " 'shoot',\n",
       " 'turkish',\n",
       " 'plane',\n",
       " 'reply',\n",
       " 'dept',\n",
       " 'science',\n",
       " 'student',\n",
       " 'write',\n",
       " 'great',\n",
       " 'black',\n",
       " 'sea',\n",
       " 'use',\n",
       " 'term',\n",
       " 'care']"
      ]
     },
     "execution_count": 921,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in common_words if i not in ENGLISH_STOP_WORDS][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "id": "837ff34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2272"
      ]
     },
     "execution_count": 919,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "len([i for i in common_words if i not in ENGLISH_STOP_WORDS])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f378371b",
   "metadata": {},
   "source": [
    "# TWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 908,
   "id": "b0e1d1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>edge</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>asian export fear damag japan rift mount trade...</td>\n",
       "      <td>asian export fear damag japan rift mount trade...</td>\n",
       "      <td>trade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>china daili vermin eat pct grain stock survei ...</td>\n",
       "      <td>china daili vermin eat pct grain stock survei ...</td>\n",
       "      <td>grain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>australian foreign ship ban end nsw port hit t...</td>\n",
       "      <td>australian foreign ship ban end nsw port hit t...</td>\n",
       "      <td>ship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sumitomo bank aim quick recoveri merger sumito...</td>\n",
       "      <td>sumitomo bank aim quick recoveri merger sumito...</td>\n",
       "      <td>acq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amatil propos two for bonu share issu amatil a...</td>\n",
       "      <td>amatil propos two for bonu share issu amatil a...</td>\n",
       "      <td>earn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  asian export fear damag japan rift mount trade...   \n",
       "1  china daili vermin eat pct grain stock survei ...   \n",
       "2  australian foreign ship ban end nsw port hit t...   \n",
       "3  sumitomo bank aim quick recoveri merger sumito...   \n",
       "4  amatil propos two for bonu share issu amatil a...   \n",
       "\n",
       "                                                edge intent  \n",
       "0  asian export fear damag japan rift mount trade...  trade  \n",
       "1  china daili vermin eat pct grain stock survei ...  grain  \n",
       "2  australian foreign ship ban end nsw port hit t...   ship  \n",
       "3  sumitomo bank aim quick recoveri merger sumito...    acq  \n",
       "4  amatil propos two for bonu share issu amatil a...   earn  "
      ]
     },
     "execution_count": 908,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "id": "9be2b5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/swifter/swifter.py:37: UserWarning: This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\n",
      "  \"This pandas object has duplicate indices, and swifter may not be able to improve performance. Consider resetting the indices with `df.reset_index(drop=True)`.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17475efe6ef34e54b76cd1e5f873c352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/7126 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1 = pd.read_csv('/home/ec2-user/SageMaker/github/aspect_topic_modeling/data/external/r8-test-stemmed.csv')\n",
    "df2 = pd.read_csv('/home/ec2-user/SageMaker/github/aspect_topic_modeling/data/external/r8-train-stemmed.csv')\n",
    "df = df1.append(df2)\n",
    "df['clean_text']  = df.swifter.apply(lambda x: x['text'].split(), axis = 1)\n",
    "common_words_ct = Counter([j for i in df['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 200)\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)}\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)}\n",
    "df['index_num'] = df.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n",
    "df['class'] = df['intent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "id": "f73eead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/R8/text.txt',\n",
    "           df['text'].values,\n",
    "           fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "3b4cad09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change it to any location you save your embeddings\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/R8/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = df, common_words = common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905decc1",
   "metadata": {},
   "source": [
    "# AgNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "5489b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "id": "34ff47b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 240794),\n",
       " ('be', 180408),\n",
       " ('a', 127679),\n",
       " ('to', 124248),\n",
       " ('of', 114080),\n",
       " ('and', 95818),\n",
       " ('i', 87457),\n",
       " ('in', 81670),\n",
       " ('that', 65129),\n",
       " ('ax', 62473),\n",
       " ('it', 59120),\n",
       " ('have', 51881),\n",
       " ('for', 46520),\n",
       " ('you', 44592),\n",
       " ('from', 37389),\n",
       " ('s', 36439),\n",
       " ('edu', 35703),\n",
       " ('on', 33701),\n",
       " ('this', 32813),\n",
       " ('not', 31746),\n",
       " ('do', 30519),\n",
       " ('t', 30325),\n",
       " ('with', 28671),\n",
       " ('or', 24298),\n",
       " ('1', 23600),\n",
       " ('if', 22625),\n",
       " ('but', 21995),\n",
       " ('line', 21842),\n",
       " ('they', 21778),\n",
       " ('subject', 20535),\n",
       " ('com', 20223),\n",
       " ('can', 20097),\n",
       " ('m', 19432),\n",
       " ('organization', 18952),\n",
       " ('at', 18496),\n",
       " ('by', 18343),\n",
       " ('re', 17622),\n",
       " ('2', 16450),\n",
       " ('what', 16449),\n",
       " ('an', 16420),\n",
       " ('my', 15915),\n",
       " ('there', 15886),\n",
       " ('one', 15389),\n",
       " ('all', 15318),\n",
       " ('would', 14825),\n",
       " ('we', 14713),\n",
       " ('0', 14691),\n",
       " ('write', 14659),\n",
       " ('will', 14640),\n",
       " ('use', 14286),\n",
       " ('3', 14169),\n",
       " ('he', 13783),\n",
       " ('about', 13765),\n",
       " ('get', 13476),\n",
       " ('x', 13347),\n",
       " ('so', 12698),\n",
       " ('article', 12694),\n",
       " ('no', 12623),\n",
       " ('your', 12316),\n",
       " ('post', 11911),\n",
       " ('say', 11805),\n",
       " ('any', 11540),\n",
       " ('c', 11517),\n",
       " ('me', 11203),\n",
       " ('u', 10995),\n",
       " ('know', 10945),\n",
       " ('who', 10944),\n",
       " ('some', 10935),\n",
       " ('out', 10375),\n",
       " ('which', 10203),\n",
       " ('don', 9966),\n",
       " ('like', 9927),\n",
       " ('make', 9882),\n",
       " ('go', 9839),\n",
       " ('more', 9712),\n",
       " ('think', 9658),\n",
       " ('4', 9564),\n",
       " ('people', 9561),\n",
       " ('university', 9516),\n",
       " ('when', 9392),\n",
       " ('just', 9370),\n",
       " ('their', 9095),\n",
       " ('5', 8956),\n",
       " ('up', 8906),\n",
       " ('d', 8799),\n",
       " ('how', 8572),\n",
       " ('other', 8530),\n",
       " ('host', 8463),\n",
       " ('time', 8248),\n",
       " ('only', 8205),\n",
       " ('them', 8154),\n",
       " ('nntp', 8114),\n",
       " ('good', 7983),\n",
       " ('than', 7838),\n",
       " ('his', 7556),\n",
       " ('7', 7549),\n",
       " ('q', 7548),\n",
       " ('see', 7376),\n",
       " ('also', 7249),\n",
       " ('_', 7226)]"
      ]
     },
     "execution_count": 940,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_ct.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 942,
   "id": "3262e4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8188.95"
      ]
     },
     "execution_count": 942,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] * 0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 945,
   "id": "27bbed98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 945,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'good' in common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "c39c61a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = [i for i in common_words if i not in ENGLISH_STOP_WORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "7dadfe3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2381"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([ i for i in common_words if i not in stopwords.words('english')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b14fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data import\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/yumeng5/WeSTClass/master/agnews/dataset.csv', \n",
    "                   error_bad_lines=False,\n",
    "                   names = ['class', 'title', 'description'])\n",
    "df['text'] = df.apply(lambda x: ' '.join(remove_stopWords(x['title'] + x['description'])), axis=1)\n",
    "df['clean_text']  = df.apply(lambda x: x['text'].split(), axis = 1)\n",
    "#get clean data\n",
    "common_words_ct = Counter([j for i in df['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 100)\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)}\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)}\n",
    "df['index_num'] = df.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change it to any location you save your embeddings\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/agnews/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = df, common_words = common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 909,
   "id": "e6d67bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>train</th>\n",
       "      <th>class</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>index_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fast cut protocol agent coordination</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[fast, cut, protocol, agent, coordination]</td>\n",
       "      <td>[0, 1, 2, 3, 4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>retrieval base class svm</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>[retrieval, base, class, svm]</td>\n",
       "      <td>[5, 6, 7, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>semantic annotation personal video content image</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>[semantic, annotation, personal, video, conten...</td>\n",
       "      <td>[9, 10, 11, 12, 13, 14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>semantic repository modeling image database</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>[semantic, repository, modeling, image, database]</td>\n",
       "      <td>[9, 15, 14, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>global local scheme imbalanced point matching</td>\n",
       "      <td>train</td>\n",
       "      <td>2</td>\n",
       "      <td>[global, local, scheme, imbalanced, point, mat...</td>\n",
       "      <td>[17, 18, 19, 20, 21]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text  train  class  \\\n",
       "0              fast cut protocol agent coordination  train      1   \n",
       "1                          retrieval base class svm  train      1   \n",
       "2  semantic annotation personal video content image  train      2   \n",
       "3       semantic repository modeling image database  train      2   \n",
       "4     global local scheme imbalanced point matching  train      2   \n",
       "\n",
       "                                          clean_text                index_num  \n",
       "0         [fast, cut, protocol, agent, coordination]          [0, 1, 2, 3, 4]  \n",
       "1                      [retrieval, base, class, svm]             [5, 6, 7, 8]  \n",
       "2  [semantic, annotation, personal, video, conten...  [9, 10, 11, 12, 13, 14]  \n",
       "3  [semantic, repository, modeling, image, database]          [9, 15, 14, 16]  \n",
       "4  [global, local, scheme, imbalanced, point, mat...     [17, 18, 19, 20, 21]  "
      ]
     },
     "execution_count": 909,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "7b9f3540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeab11ca6834491fb99f587c821a1716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/54595 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "news = pd.read_csv('/home/ec2-user/SageMaker/github/aspect_topic_modeling/data/external/corpusbibitex.tsv',  sep = '\\t',\n",
    "                   error_bad_lines=False,\n",
    "                   names = ['text', 'train', 'class'])\n",
    "#change it to any location you save your embeddings\n",
    "#news = news.groupby('class').head(4500)\n",
    "news['clean_text']  = news.apply(lambda x: x['text'].split(), axis = 1)\n",
    "#get clean data\n",
    "common_words_ct = Counter([j for i in news['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 50) #this vocab\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)} #word dict\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)} \n",
    "news['index_num'] = news.swifter.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n",
    "\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/bibi/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = news, common_words = common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c0efdf",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f124a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13230412471889824"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> from sklearn import metrics\n",
    ">>> labels_true = [1, 1, 1, 0, 0, 0]\n",
    ">>> labels_pred = [0, 2, 2, 2, 2, 2]\n",
    "\n",
    ">>> metrics.mutual_info_score(labels_true, labels_pred)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54661cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4620981203732969"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = [3, 3, 1, 1, 2, 2]\n",
    "metrics.mutual_info_score(labels_true, labels_pred)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1097aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    " \n",
    "def most_frequent(List):\n",
    "    occurence_count = Counter(List)\n",
    "    return occurence_count.most_common(1)[0][1]\n",
    "   \n",
    "List = [2, 1, 2, 2, 1, 3]\n",
    "print(most_frequent(List))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "c34ee271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def top_purity(labels_true, labels_pred):\n",
    "    pred = np.unique(labels_pred)\n",
    "    d = defaultdict(list)\n",
    "    for i, j in zip(labels_pred, labels_true):\n",
    "        d[i].append(j)\n",
    "    ct = []\n",
    "    for i in d:\n",
    "        ct += [Counter(d[i]).most_common(1)[0][1]]\n",
    "        #print(Counter(d[i]).most_common(1)[0], i, len(d[i]))\n",
    "    print(ct)\n",
    "    return np.sum(ct)/len(labels_pred)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a1f95252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[952, 629, 18, 23, 9, 10, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1646"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_purity(labels_true, labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0bb1f1",
   "metadata": {},
   "source": [
    "# VNTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "9516ebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_embedding_weighted_penalty(embedding_weight, topic_word_logit, EPS=1e-12):\n",
    "    \"\"\"embedding_weight: V x dim, topic_word_logit: T x V.\"\"\"\n",
    "    #add importance to each word\n",
    "    w = topic_word_logit.transpose(0, 1)  # V x T\n",
    "    #normalized embeddings\n",
    "    nv = embedding_weight / (torch.norm(embedding_weight, dim=1, keepdim=True) + EPS)  # V x dim\n",
    "    #normalized importance\n",
    "    nw = w / (torch.norm(w, dim=0, keepdim=True) + EPS)  # V x T\n",
    "    #get topic representation\n",
    "    t = nv.transpose(0, 1) @ w  # dim x T\n",
    "    nt = t / (torch.norm(t, dim=0, keepdim=True) + EPS)  # dim x T\n",
    "    #word - topics similarity matrix\n",
    "    s = nv @ nt  # V x T\n",
    "    #we want normalized importance is closed their similarity\n",
    "    return -(s * nw).sum()  # minus for minimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "id": "41521d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VNTM(nn.Module):\n",
    "    \"\"\"NTM that keeps track of output\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, normal, h_to_z, topics, layer, top_number, penalty):\n",
    "        super(VNTM, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        #self.normal = normal\n",
    "        self.h_to_z = h_to_z\n",
    "        self.topics = topics\n",
    "        self.output = None\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        self.fc_mean = nn.Linear(layer, top_number)\n",
    "        self.fc_var = nn.Linear(layer, 1)\n",
    "        self.num = top_number\n",
    "        self.penalty = penalty\n",
    "        #self.dirichlet = torch.distributions.dirichlet.Dirichlet((torch.ones(self.topics.k)/self.topics.k).cuda())\n",
    "    def forward(self, x, n_sample=1, epoch = 0):\n",
    "        h = self.hidden(x)\n",
    "        h = self.drop(h)\n",
    "        z_mean = self.fc_mean(h)\n",
    "        z_mean = z_mean / z_mean.norm(dim=-1, keepdim=True)\n",
    "        # the `+ 1` prevent collapsing behaviors\n",
    "        z_var = F.softplus(self.fc_var(h)) + 1\n",
    "        \n",
    "        q_z = VonMisesFisher(z_mean, z_var)\n",
    "        p_z = HypersphericalUniform(self.num - 1, device=device)\n",
    "        kld = torch.distributions.kl.kl_divergence(q_z, p_z).mean()\n",
    "        #print(q_z)\n",
    "        #mu, log_sigma = self.normal(h)\n",
    "        #identify how far it is away from normal distribution\n",
    "        \n",
    "        #print(kld.shape)\n",
    "        rec_loss = 0\n",
    "        for i in range(n_sample):\n",
    "            #reparametrician trick\n",
    "            z = q_z.rsample()\n",
    "            #z = nn.Softmax()(z)\n",
    "            #decode\n",
    "            #print(z)\n",
    "            \n",
    "            z = self.h_to_z(10 * z)\n",
    "            self.output = z\n",
    "            #print(z)\n",
    "            \n",
    "            #get log probability for reconstruction loss\n",
    "            log_prob = self.topics(z)\n",
    "            rec_loss = rec_loss - (log_prob * x).sum(dim=-1)\n",
    "        #average reconstruction loss\n",
    "        rec_loss = rec_loss / n_sample\n",
    "        #print(rec_loss.shape)\n",
    "        minus_elbo = rec_loss + kld\n",
    "        penalty, var, mean = topic_covariance_penalty(self.topics.topic_emb) \n",
    "        return {\n",
    "            'loss': minus_elbo + penalty * self.penalty,\n",
    "            'minus_elbo': minus_elbo,\n",
    "            'rec_loss': rec_loss,\n",
    "            'kld': kld\n",
    "        }\n",
    "\n",
    "    def get_topics(self):\n",
    "        return self.topics.get_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "da86da99",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'EmbTopic' object has no attribute 'get_topic_word_logit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-608-19ac4285e45e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_topic_word_logit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    777\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 779\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleAttributeError\u001b[0m: 'EmbTopic' object has no attribute 'get_topic_word_logit'"
     ]
    }
   ],
   "source": [
    "model.topics.get_topic_word_logit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "id": "ddb66e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, batch_size, epoch, optimizer, scheduler):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    total_nll = 0.0\n",
    "    total_kld = 0.0\n",
    "    total_words = 0\n",
    "    total_penalty = 0.0\n",
    "    #size = epoch_size * batch_size\n",
    "    indices = torch.randperm(X.shape[0])\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    #print(indices)\n",
    "    length = len(indices)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = torch.from_numpy(X[ind].toarray()).float().to(device)\n",
    "        \n",
    "        d = model(x = data_batch)\n",
    "            \n",
    "        \n",
    "        \n",
    "        total_nll += d['rec_loss'].sum().item() / batch_size\n",
    "        total_kld += d['kld'].sum().item() / batch_size  \n",
    "        #total_penalty += d['prior']  \n",
    "#         if i < 3:\n",
    "#             loss = d['minus_elbo']\n",
    "#         else:\n",
    "        loss = d['loss']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    print(total_nll/length, total_kld/length)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "55a8ebbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7126, 363)"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "7d118d30",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.631355989759214 0.00018478308840509\n",
      "34.5023542163528 0.00018801333730339691\n",
      "33.57135835540629 0.0009436759800991809\n",
      "32.72929679567569 0.0021514606588111025\n",
      "32.329653472543875 0.002785144748044348\n",
      "32.10514688046179 0.0032073157278072334\n",
      "31.947803755786932 0.003495215552545666\n",
      "31.87388635795807 0.0036559811353718288\n",
      "31.81700792045237 0.0038143141558567915\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-905-dc98bb6711ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-709-ae882023c876>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, batch_size, epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-708-112ffaf082b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, n_sample, epoch)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#print(rec_loss.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mminus_elbo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrec_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkld\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_covariance_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         return {\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mminus_elbo\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpenalty\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/github/aspect_topic_modeling/models/NVDM.py\u001b[0m in \u001b[0;36mtopic_covariance_penalty\u001b[0;34m(topic_emb, EPS)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m#normalized the topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnormalized_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_emb\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mEPS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;31m#get topic similarity absolute value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnormalized_topic\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mnormalized_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m#average similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "            gc.collect()\n",
    "            numb_embeddings = 20\n",
    "            hidden = get_mlp([X.shape[1], 128], nn.ReLU)\n",
    "            normal = NormalParameter(128, 20)\n",
    "            h_to_z = nn.Softmax()\n",
    "            embedding = nn.Embedding(X.shape[1], 20)\n",
    "            # p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "            # out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "            embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "            embedding.weight.requires_grad=False\n",
    "            topics = EmbTopic(embedding = embedding,\n",
    "                              k = numb_embeddings, normalize = False)\n",
    "\n",
    "\n",
    "\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "            model1 = VNTM(hidden = hidden,\n",
    "                        normal = normal,\n",
    "                        h_to_z = h_to_z,\n",
    "                        topics = topics,\n",
    "                        layer = 128, \n",
    "                        top_number = numb_embeddings,\n",
    "                        penalty = 1\n",
    "                        ).to(device).float()\n",
    "            # larger hidden size make topics more diverse\n",
    "            #num_docs_train = 996318\n",
    "            batch_size = 256\n",
    "            optimizer = optim.Adam(model1.parameters(), \n",
    "                                   lr=0.002, \n",
    "                                   weight_decay=1.2e-6)\n",
    "\n",
    "            epochs = 20\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=epochs)\n",
    "\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                train(model1, X,  batch_size, epoch, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e0d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "e6b32d86",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   3362\u001b[0m         \"\"\"\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Maybe set copy if we didn't actually change the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3350\u001b[0m         new_data = self._mgr.take(\n\u001b[0;32m-> 3351\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3352\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-898-6e774155ab8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1485\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1489\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "df = news.iloc[indices]\n",
    "df.iloc[indices].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "9b0ce9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[420, 2390, 288, 2442, 880, 4503, 3407, 2868, 3173, 476, 4223, 3938, 667, 372, 3554, 530, 136, 166, 78, 152]\n",
      "[2036, 3403, 1775, 3294, 2615, 2564, 1066, 1096, 2676, 829, 852, 1047, 2566, 601, 2996, 666, 1068, 2476, 790, 768]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.634934881761398,\n",
       " 0.17736129097988454,\n",
       " 0.6444782298096826,\n",
       " 0.17401671830334664)"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prior(softmax_top, indexes)\n",
    "emb = model1.topics.get_topics().cpu().detach().numpy()\n",
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:25] ] for i in range(numb_embeddings)]\n",
    "data_batch = torch.from_numpy(X.toarray()).float()\n",
    "model1.cpu()\n",
    "z = model1.hidden(data_batch)\n",
    "z_mean = model1.fc_mean(z)\n",
    "z_mean = z_mean / z_mean.norm(dim=-1, keepdim=True)\n",
    "z = model1.h_to_z(z_mean)\n",
    "\n",
    "\n",
    "labels_pred = torch.argmax(z, 1).numpy()\n",
    "labels_true = df['class'].values\n",
    " \n",
    "\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "#coherence_score = get_topic_coherence(X.toarray(), topics, word_track)\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=numb_embeddings, random_state=0).fit(z.detach().numpy())\n",
    "top_purity(labels_true, labels_pred), metrics.normalized_mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.normalized_mutual_info_score(labels_true, kmeans.labels_)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "f23d663a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.0,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.04,\n",
       "  0.08,\n",
       "  0.08,\n",
       "  0.36,\n",
       "  0.48,\n",
       "  0.48,\n",
       "  0.52,\n",
       "  0.56,\n",
       "  0.6,\n",
       "  0.64,\n",
       "  0.68,\n",
       "  0.72,\n",
       "  0.8,\n",
       "  0.84],\n",
       " 0.352)"
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:25] ] for i in range(numb_embeddings)]\n",
    "sorted(diversity(topics)), np.mean(diversity(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "74d71ea7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5040000000000001, -0.40466082922081026)"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=20, random_state=0)\n",
    "lda.fit(X)\n",
    "topic_index = np.argsort(lda.components_, axis=1)[:, -25:]\n",
    "topics = [[index_track[j] for j in i] for i in topic_index]\n",
    "np.mean(diversity(topics)), np.mean(get_topic_coherence(X.toarray(), topics, word_track))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "f0febcc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['stg',\n",
       "  'rise',\n",
       "  'final',\n",
       "  'turnov',\n",
       "  'plc',\n",
       "  'improv',\n",
       "  'strong',\n",
       "  'payabl',\n",
       "  'div',\n",
       "  'pretax',\n",
       "  'intern',\n",
       "  'extraordinari',\n",
       "  'dilut',\n",
       "  'show',\n",
       "  'adjust',\n",
       "  'subject',\n",
       "  'nil',\n",
       "  'incom',\n",
       "  'result',\n",
       "  'posit',\n",
       "  'properti',\n",
       "  'make',\n",
       "  'cent',\n",
       "  'european',\n",
       "  'long'],\n",
       " ['year',\n",
       "  'dlr',\n",
       "  'profit',\n",
       "  'mln',\n",
       "  'billion',\n",
       "  'earn',\n",
       "  'net',\n",
       "  'compar',\n",
       "  'quarter',\n",
       "  'sale',\n",
       "  'revenu',\n",
       "  'loss',\n",
       "  'oper',\n",
       "  'estim',\n",
       "  'last',\n",
       "  'expect',\n",
       "  'earlier',\n",
       "  'result',\n",
       "  'compani',\n",
       "  'ago',\n",
       "  'fiscal',\n",
       "  'asset',\n",
       "  'provis',\n",
       "  'total',\n",
       "  'incom'],\n",
       " ['york',\n",
       "  'system',\n",
       "  'cost',\n",
       "  'energi',\n",
       "  'area',\n",
       "  'revenu',\n",
       "  'estim',\n",
       "  'servic',\n",
       "  'monei',\n",
       "  'incom',\n",
       "  'file',\n",
       "  'fiscal',\n",
       "  'fourth',\n",
       "  'offic',\n",
       "  'manag',\n",
       "  'provid',\n",
       "  'outstand',\n",
       "  'line',\n",
       "  'depart',\n",
       "  'feder',\n",
       "  'texa',\n",
       "  'account',\n",
       "  'western',\n",
       "  'recent',\n",
       "  'problem'],\n",
       " ['share',\n",
       "  'inc',\n",
       "  'offer',\n",
       "  'common',\n",
       "  'corp',\n",
       "  'sharehold',\n",
       "  'acquir',\n",
       "  'compani',\n",
       "  'outstand',\n",
       "  'bui',\n",
       "  'group',\n",
       "  'stock',\n",
       "  'dlr',\n",
       "  'purchas',\n",
       "  'subsidiari',\n",
       "  'hold',\n",
       "  'reuter',\n",
       "  'manag',\n",
       "  'tender',\n",
       "  'secur',\n",
       "  'transact',\n",
       "  'acquisit',\n",
       "  'stake',\n",
       "  'propos',\n",
       "  'cash'],\n",
       " ['record',\n",
       "  'qtly',\n",
       "  'dividend',\n",
       "  'april',\n",
       "  'div',\n",
       "  'pai',\n",
       "  'quarterli',\n",
       "  'prior',\n",
       "  'payabl',\n",
       "  'payout',\n",
       "  'set',\n",
       "  'split',\n",
       "  'june',\n",
       "  'declar',\n",
       "  'march',\n",
       "  'juli',\n",
       "  'holder',\n",
       "  'reuter',\n",
       "  'annual',\n",
       "  'stock',\n",
       "  'initi',\n",
       "  'note',\n",
       "  'vote',\n",
       "  'name',\n",
       "  'board'],\n",
       " ['ship',\n",
       "  'union',\n",
       "  'talk',\n",
       "  'spokesman',\n",
       "  'plan',\n",
       "  'work',\n",
       "  'takeov',\n",
       "  'week',\n",
       "  'propos',\n",
       "  'yesterdai',\n",
       "  'british',\n",
       "  'two',\n",
       "  'last',\n",
       "  'pai',\n",
       "  'month',\n",
       "  'offici',\n",
       "  'debt',\n",
       "  'meet',\n",
       "  'western',\n",
       "  'reach',\n",
       "  'minist',\n",
       "  'corp',\n",
       "  'gulf',\n",
       "  'negoti',\n",
       "  'statement'],\n",
       " ['trade',\n",
       "  'japan',\n",
       "  'good',\n",
       "  'countri',\n",
       "  'dollar',\n",
       "  'japanes',\n",
       "  'import',\n",
       "  'foreign',\n",
       "  'deficit',\n",
       "  'exchang',\n",
       "  'market',\n",
       "  'hous',\n",
       "  'reagan',\n",
       "  'state',\n",
       "  'minist',\n",
       "  'surplu',\n",
       "  'action',\n",
       "  'industri',\n",
       "  'tariff',\n",
       "  'offici',\n",
       "  'european',\n",
       "  'currenc',\n",
       "  'pact',\n",
       "  'govern',\n",
       "  'econom'],\n",
       " ['seek',\n",
       "  'activ',\n",
       "  'anoth',\n",
       "  'public',\n",
       "  'texa',\n",
       "  'mth',\n",
       "  'market',\n",
       "  'relat',\n",
       "  'todai',\n",
       "  'action',\n",
       "  'rais',\n",
       "  'earli',\n",
       "  'cent',\n",
       "  'jan',\n",
       "  'continu',\n",
       "  'posit',\n",
       "  'discuss',\n",
       "  'negoti',\n",
       "  'chang',\n",
       "  'oper',\n",
       "  'demand',\n",
       "  'inc',\n",
       "  'talk',\n",
       "  'week',\n",
       "  'transact'],\n",
       " ['mln',\n",
       "  'net',\n",
       "  'loss',\n",
       "  'shr',\n",
       "  'dlr',\n",
       "  'profit',\n",
       "  'qtr',\n",
       "  'avg',\n",
       "  'rev',\n",
       "  'oper',\n",
       "  'mth',\n",
       "  'year',\n",
       "  'note',\n",
       "  'reuter',\n",
       "  'corp',\n",
       "  'sale',\n",
       "  'exclud',\n",
       "  'includ',\n",
       "  'billion',\n",
       "  'inc',\n",
       "  'tax',\n",
       "  'discontinu',\n",
       "  'jan',\n",
       "  'share',\n",
       "  'extraordinari'],\n",
       " ['oil',\n",
       "  'barrel',\n",
       "  'bank',\n",
       "  'rate',\n",
       "  'crude',\n",
       "  'dai',\n",
       "  'price',\n",
       "  'averag',\n",
       "  'bpd',\n",
       "  'petroleum',\n",
       "  'rais',\n",
       "  'central',\n",
       "  'saudi',\n",
       "  'pct',\n",
       "  'rise',\n",
       "  'monei',\n",
       "  'opec',\n",
       "  'cut',\n",
       "  'market',\n",
       "  'sourc',\n",
       "  'earli',\n",
       "  'thi',\n",
       "  'half',\n",
       "  'level',\n",
       "  'output']]"
      ]
     },
     "execution_count": 860,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c2fb97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a350789",
   "metadata": {},
   "source": [
    "# NTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "977b2493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, batch_size, epoch, optimizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    total_nll = 0.0\n",
    "    total_kld = 0.0\n",
    "    total_words = 0\n",
    "    total_penalty = 0.0\n",
    "    #size = epoch_size * batch_size\n",
    "    indices = torch.randperm(X.shape[0])\n",
    "    indices = torch.split(indices, batch_size)\n",
    "    #print(indices)\n",
    "    length = len(indices)\n",
    "    for idx, ind in enumerate(indices):\n",
    "        data_batch = torch.from_numpy(X[ind].toarray()).float().to(device)\n",
    "        \n",
    "        d = model(x = data_batch)\n",
    "            \n",
    "        \n",
    "        \n",
    "        total_nll += d['rec_loss'].sum().item() / batch_size\n",
    "        total_kld += d['kld'].sum().item() / batch_size  \n",
    "        #total_penalty += d['penalty'].sum().item() / batch_size  \n",
    "#         if i < 3:\n",
    "#             loss = d['minus_elbo']\n",
    "#         else:\n",
    "        loss = d['loss']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(total_nll/length, total_kld/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "f26872a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0005, 0.0005, 0.0004,  ..., 0.0003, 0.0004, 0.0005],\n",
       "        [0.0005, 0.0004, 0.0004,  ..., 0.0004, 0.0005, 0.0004],\n",
       "        [0.0003, 0.0003, 0.0005,  ..., 0.0005, 0.0005, 0.0005],\n",
       "        ...,\n",
       "        [0.0004, 0.0005, 0.0003,  ..., 0.0004, 0.0005, 0.0004],\n",
       "        [0.0004, 0.0005, 0.0005,  ..., 0.0003, 0.0004, 0.0004],\n",
       "        [0.0005, 0.0004, 0.0004,  ..., 0.0005, 0.0004, 0.0004]])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = Topics(numb_embeddings, X.shape[1])\n",
    "topics.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "49888624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "836.3474549989443 0.1607032038548307\n",
      "819.3788678968275 0.03806465583526202\n",
      "806.9361918681377 0.03929201230985691\n",
      "797.9546310837204 0.048760768302993196\n",
      "791.4055390229096 0.04600683428548478\n",
      "786.7550077696104 0.038150564075221084\n",
      "783.4681206780512 0.02321837155299412\n",
      "781.0415558686127 0.025440513194110746\n",
      "779.4308673755543 0.04452345746481237\n",
      "778.4024130331503 0.0753821546048228\n",
      "777.6905938225824 0.06864226998987834\n",
      "777.3639212943413 0.023134106206959364\n",
      "776.9615222827808 0.1023010356647491\n",
      "776.8203562143686 0.06381778039881406\n",
      "776.6001384838207 0.0499157341703697\n",
      "776.7200680294552 0.10331941790236915\n",
      "776.361110377956 0.13221442471793224\n",
      "776.5740822456978 0.11337339315198462\n",
      "776.1798528722815 0.162217427351877\n",
      "776.2847937506598 0.10506580079704322\n"
     ]
    }
   ],
   "source": [
    "from models.NVDM import NTM\n",
    "import gc\n",
    "#labels = [[120], [1527], [1646], [2047], [727], [1624], [36], [32], [26], [92], [907], [652]]\n",
    "numb_embeddings = 20\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1], 64], nn.ReLU)\n",
    "normal = NormalParameter(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "# embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "# embedding.weight.requires_grad=False\n",
    "topics = Topics(numb_embeddings, X.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = NTM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics\n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.05, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "63a1fbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[964, 624, 19, 21, 9, 6, 1, 1, 1, 1]\n",
      "[981, 67, 121, 14, 51, 58, 81, 120, 14, 8, 18, 4, 21, 12, 29, 7, 23, 4, 13, 5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08739255014326648,\n",
       " 0.09698613336289995,\n",
       " 0.08760479677385122,\n",
       " 0.12068967614918263)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = model.hidden(data_batch)\n",
    "h = model.drop(h)\n",
    "mu, log_sigma = model.normal(h)\n",
    "z = model.h_to_z(mu)\n",
    "labels_pred = torch.argmax(z, 1).numpy()\n",
    "labels_true = df['class'].values\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(z.detach().numpy())\n",
    "top_purity(labels_true, labels_pred), metrics.mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.mutual_info_score(labels_true, kmeans.labels_)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c243e3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.64, 0.64, 0.72, 1.0, 0.84, 1.0, 0.88, 0.68, 0.76, 0.92]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "33b6de74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['from', 'mamatha', 'devineni', 'ratnam', 'mr47', 'andrew', 'cmu', 'edu', 'subject', 'pen', 'fan', 'reaction', 'organization', 'post', 'office', 'carnegie', 'mellon', 'pittsburgh', 'pa', 'line', '12', 'nntp', 'post', 'host', 'po4', 'andrew', 'cmu', 'edu', 'i', 'be', 'sure', 'some', 'bashers', 'of', 'pen', 'fan', 'be', 'pretty', 'confuse', 'about', 'the', 'lack', 'of', 'any', 'kind', 'of', 'post', 'about', 'the', 'recent', 'pen', 'massacre', 'of', 'the', 'devil', 'actually', 'i', 'be', 'bit', 'puzzled', 'too', 'and', 'a', 'bit', 'relieved', 'however', 'i', 'be', 'go', 'to', 'put', 'an', 'end', 'to', 'non', 'pittsburghers', 'relief', 'with', 'a', 'bit', 'of', 'praise', 'for', 'the', 'pen', 'man', 'they', 'be', 'kill', 'those', 'devil', 'bad', 'than', 'i', 'think', 'jagr', 'just', 'show', 'you', 'why', 'he', 'be', 'much', 'good', 'than', 'his', 'regular', 'season', 'stats', 'he', 'be', 'also', 'a', 'lot', 'fo', 'fun', 'to', 'watch', 'in', 'the', 'playoff', 'bowman', 'should', 'let', 'jagr', 'have', 'a', 'lot', 'of', 'fun', 'in', 'the', 'next', 'couple', 'of', 'game', 'since', 'the', 'pen', 'be', 'go', 'to', 'beat', 'the', 'pulp', 'out', 'of', 'jersey', 'anyway', 'i', 'be', 'very', 'disappointed', 'not', 'to', 'see', 'the', 'islander', 'lose', 'the', 'final', 'regular', 'season', 'game', 'pen', 'rule']),\n",
       "       list(['from', 'mblawson', 'midway', 'ecn', 'uoknor', 'edu', 'matthew', 'b', 'lawson', 'subject', 'which', 'high', 'performance', 'vlb', 'video', 'card', 'summary', 'seek', 'recommendation', 'for', 'vlb', 'video', 'card', 'nntp', 'post', 'host', 'midway', 'ecn', 'uoknor', 'edu', 'organization', 'engineering', 'computer', 'network', 'university', 'of', 'oklahoma', 'norman', 'ok', 'usa', 'keywords', 'orchid', 'stealth', 'vlb', 'line', '21', 'my', 'brother', 'be', 'in', 'the', 'market', 'for', 'a', 'high', 'performance', 'video', 'card', 'that', 'support', 'vesa', 'local', 'bus', 'with', '1', '2mb', 'ram', 'do', 'anyone', 'have', 'suggestion', 'idea', 'on', 'diamond', 'stealth', 'pro', 'local', 'bus', 'orchid', 'farenheit', '1280', 'ati', 'graphic', 'ultra', 'pro', 'any', 'other', 'high', 'performance', 'vlb', 'card', 'please', 'post', 'or', 'email', 'thank', 'you', 'matt', 'matthew', 'b', 'lawson', 'mblawson', 'essex', 'ecn', 'uoknor', 'edu', 'now', 'i', 'nebuchadnezzar', 'praise', 'and', 'exalt', 'and', 'glorify', 'the', 'king', 'of', 'heaven', 'because', 'everything', 'he', 'do', 'be', 'right', 'and', 'all', 'his', 'way', 'be', 'just', 'nebuchadnezzar', 'king', 'of', 'babylon', '562', 'b', 'c'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9368d6b0",
   "metadata": {},
   "source": [
    "# GSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "dfb55c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSM(NTM):\n",
    "    def __init__(self, hidden, normal, h_to_z, topics, penalty):\n",
    "        # h_to_z will output probabilities over topics\n",
    "        super(GSM, self).__init__(hidden, normal, h_to_z, topics)\n",
    "        self.penalty = penalty\n",
    "\n",
    "    def forward(self, x, n_sample=1):\n",
    "        stat = super(GSM, self).forward(x, n_sample)\n",
    "        loss = stat['loss']\n",
    "        penalty, var, mean = topic_covariance_penalty(self.topics.topic_emb)\n",
    "\n",
    "        stat.update({\n",
    "            'loss': loss + penalty * self.penalty,\n",
    "            'penalty_mean': mean,\n",
    "            'penalty_var': var,\n",
    "            'penalty': penalty * self.penalty,\n",
    "        })\n",
    "\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "0787f261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2394, 50])"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "137e092b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/github/aspect_topic_modeling/models/NVDM.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  z = self.h_to_z(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215.70148304530554 0.14776690251060895\n",
      "214.76792471749442 0.1005628505455596\n",
      "211.87730516706193 0.23094200475939683\n",
      "205.10287530081612 0.9738367817231587\n",
      "195.9407343183245 1.8370886487620217\n",
      "190.70264598301478 2.188159533909389\n",
      "188.08613804408483 2.5685715675354004\n",
      "186.05395344325476 3.0157410332134793\n",
      "184.89676230294364 3.212756190981184\n",
      "184.2214072091239 3.352269853864397\n",
      "183.68893814086914 3.470014682837895\n",
      "183.20615713936942 3.5006412523133412\n",
      "182.8960827418736 3.618808550494058\n",
      "182.43153599330358 3.6915909137044634\n",
      "182.3068389892578 3.723042828696115\n",
      "182.09178924560547 3.8733092716761996\n",
      "181.99352645874023 3.77488249540329\n",
      "182.0485714503697 3.7913839391299655\n",
      "181.976445879255 3.786278716155461\n",
      "181.8711782182966 3.839538880756923\n"
     ]
    }
   ],
   "source": [
    "from models.NVDM import NTM\n",
    "import gc\n",
    "#labels = [[120], [1527], [1646], [2047], [727], [1624], [36], [32], [26], [92], [907], [652]]\n",
    "numb_embeddings = 10\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1], 64], nn.ReLU)\n",
    "normal = NormalParameter(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "embedding.weight = torch.nn.Parameter(torch.ones(embed.shape))\n",
    "# embedding.weight.requires_grad=False\n",
    "#embedding.weight = torch.nn.Parameter()\n",
    "embedding.weight.requires_grad=True\n",
    "topics = EmbTopic(embedding = embedding,\n",
    "                  \n",
    "                  k = numb_embeddings, normalize = False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = GSM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics,\n",
    "            penalty = 10\n",
    "        \n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "5bc40ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333, 295, 1755, 2474, 53, 672, 4, 4]\n",
      "[90, 454, 95, 210, 243, 332, 266, 80, 518, 70, 345, 214, 70, 239, 620, 679, 509, 239, 154, 194]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7844513050799887,\n",
       " 0.49100081796451894,\n",
       " 0.7888015717092338,\n",
       " 0.3558329818145496)"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prior(softmax_top, indexes)\n",
    "emb = model.topics.get_topics().cpu().detach().numpy()\n",
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:25] ] for i in range(numb_embeddings)]\n",
    "data_batch = torch.from_numpy(X.toarray()).float()\n",
    "model.cpu()\n",
    "h = model.hidden(data_batch)\n",
    "h = model.drop(h)\n",
    "mu, log_sigma = model.normal(h)\n",
    "z = model.h_to_z(mu)\n",
    "labels_pred = torch.argmax(z, 1).numpy()\n",
    "labels_true = df['class'].values\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(z.detach().numpy())\n",
    "top_purity(labels_true, labels_pred), metrics.normalized_mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.normalized_mutual_info_score(labels_true, kmeans.labels_)  \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "1cfebadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22000000000000003"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(diversity(topics))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "d757a313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/github/aspect_topic_modeling/models/NVDM.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  z = self.h_to_z(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.4660633632115 0.3990607889635222\n",
      "217.10845238821847 0.3413937655942781\n",
      "216.30051204136439 0.4339985890047891\n",
      "215.06954792567663 0.8085194570677621\n",
      "213.8268165588379 1.0948167613574438\n",
      "212.7809317452567 1.3196682802268438\n",
      "211.70422199794226 1.4821047612598963\n",
      "210.70782198224748 1.6934643260070257\n",
      "209.74313735961914 1.9047679560525077\n",
      "208.99157987322127 1.973192376749856\n",
      "208.32663508823939 2.115339517593384\n",
      "207.69187600272042 2.235893888132913\n",
      "207.15857751028878 2.372569982494627\n",
      "206.86663436889648 2.4393067998545512\n",
      "206.5529294695173 2.4425294484410967\n",
      "206.3272329057966 2.4712788505213603\n",
      "206.23358481270927 2.480587669781276\n",
      "206.1489655630929 2.534112742968968\n",
      "206.18519701276506 2.5088083148002625\n",
      "206.08026776994978 2.518525685582842\n"
     ]
    }
   ],
   "source": [
    "from models.NVDM import NTM\n",
    "import gc\n",
    "#labels = [[120], [1527], [1646], [2047], [727], [1624], [36], [32], [26], [92], [907], [652]]\n",
    "numb_embeddings = 20\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1],  64], nn.ReLU)\n",
    "normal = NormalParameter(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "# embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "# embedding.weight.requires_grad=False\n",
    "embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "embedding.weight.requires_grad=False\n",
    "topics = EmbTopic(embedding = embedding,\n",
    "                  k = numb_embeddings, normalize = False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = GSM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics,\n",
    "            penalty = 0\n",
    "        \n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer, scheduler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "4f3ce7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94, 5, 390, 51, 3477, 135, 171, 12, 251, 23, 82, 19, 7, 2, 1, 1]\n",
      "[27, 1088, 114, 466, 531, 78, 747, 405, 204, 337, 267, 66, 269, 19, 27, 9, 29, 21, 11, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6625035082795397,\n",
       " 0.34369873489030883,\n",
       " 0.6625035082795397,\n",
       " 0.21958569048131982)"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prior(softmax_top, indexes)\n",
    "emb = model.topics.get_topics().cpu().detach().numpy()\n",
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:25] ] for i in range(20)]\n",
    "data_batch = torch.from_numpy(X.toarray()).float()\n",
    "model.cpu()\n",
    "h = model.hidden(data_batch)\n",
    "h = model.drop(h)\n",
    "mu, log_sigma = model.normal(h)\n",
    "z = model.h_to_z(mu)\n",
    "labels_pred = torch.argmax(z, 1).numpy()\n",
    "labels_true = df['class'].values\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(z.detach().numpy())\n",
    "top_purity(labels_true, labels_pred), metrics.normalized_mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.normalized_mutual_info_score(labels_true, kmeans.labels_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "df9dcade",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26200000000000007"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(diversity(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8962041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "af2173b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling/pt-avitm')\n",
    "from ptavitm.sklearn_api import ProdLDATransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "5409b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProdLDATransformer(\n",
    "        cuda=device,\n",
    "        batch_size=256,\n",
    "        epochs=80,\n",
    "        hidden1_dimension=64,\n",
    "        hidden2_dimension=64,\n",
    "        topics=20,\n",
    "        lr=0.001,\n",
    "        samples=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "78d5029a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "83cbeb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "d39ca16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        decoder_weight = model.autoencoder.decoder.linear.weight.detach().cpu()\n",
    "        id2word = {index: str(index) for index in range(X.shape[1])}\n",
    "        topics = [\n",
    "            [item.item() for item in topic]\n",
    "            for topic in decoder_weight.topk(min(model.score_num, X.shape[1]), dim=0)[\n",
    "                1\n",
    "            ].t()\n",
    "        ]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "eb928cb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.7142857142857143,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.8571428571428571,\n",
       " 1.0,\n",
       " 0.5714285714285714]"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diversity(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "0ab5395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103, 398, 243, 70, 225, 448, 265, 437, 53, 331, 336, 245, 384, 105, 182, 264, 100, 240, 344, 151]\n",
      "[98, 510, 181, 339, 225, 577, 357, 313, 315, 596, 137, 452, 479, 266, 161, 241, 251, 191, 104, 173]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2612756022498143,\n",
       " 0.24761351519096145,\n",
       " 0.3165658495171389,\n",
       " 0.3046968220454173)"
      ]
     },
     "execution_count": 556,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_pred = result.argmax(1)\n",
    "labels_true = df['class'].values\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(normalize(result, axis=1))\n",
    "top_purity(labels_true, labels_pred), metrics.normalized_mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.normalized_mutual_info_score(labels_true, kmeans.labels_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "553fff3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.matutils import Sparse2Corpus\n",
    "corpus = Sparse2Corpus(X, documents_columns=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "f9681bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1877, 1617, 2048, 1968, 1881, 1878, 1754],\n",
       " [835, 2095, 697, 1628, 2010, 711, 1833],\n",
       " [1370, 405, 1948, 1464, 2225, 1413, 2162],\n",
       " [2040, 2031, 1740, 215, 2033, 2032, 254],\n",
       " [1630, 910, 1076, 1935, 1938, 2168, 245],\n",
       " [208, 144, 1598, 1593, 1597, 1243, 1127],\n",
       " [2323, 1158, 2246, 830, 2002, 2014, 1210],\n",
       " [763, 764, 1336, 1567, 758, 462, 2159],\n",
       " [1112, 2272, 1346, 1100, 1239, 567, 1099],\n",
       " [2386, 2392, 2388, 2391, 2382, 2381, 2389],\n",
       " [1638, 1670, 1634, 1269, 2286, 166, 1671],\n",
       " [1461, 1460, 1427, 667, 1462, 1439, 1099],\n",
       " [557, 1614, 2157, 2377, 2345, 334, 1826],\n",
       " [436, 435, 438, 2175, 1034, 2258, 1986],\n",
       " [681, 1193, 2034, 599, 2239, 807, 1859],\n",
       " [1042, 1215, 1146, 1086, 1894, 1043, 1275],\n",
       " [772, 778, 1315, 774, 187, 797, 2223],\n",
       " [1742, 2088, 1744, 2039, 1562, 1745, 446],\n",
       " [1544, 1352, 417, 111, 1405, 335, 1068],\n",
       " [1562, 1542, 435, 91, 425, 438, 1541]]"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "40547217",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = Sparse2Corpus(X, documents_columns=False)\n",
    "#decoder_weight = self.autoencoder.decoder.linear.weight.detach().cpu()\n",
    "id2word = {index: str(index) for index in range(X.shape[1])}\n",
    "topics = [\n",
    "            [item.item() for item in topic]\n",
    "            for topic in decoder_weight.topk(min(model.score_num, X.shape[1]), dim=0)[\n",
    "                1\n",
    "            ].t()\n",
    "        ]\n",
    " \n",
    "cm = CoherenceModel(\n",
    "    topics=topics,\n",
    "    corpus=corpus,\n",
    "    dictionary=Dictionary.from_corpus(corpus, id2word),\n",
    "    coherence=\"u_mass\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "27b45e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04799074373478669"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(\n",
    "    topics=topics,\n",
    "    texts = df.apply(lambda x:[str(i) for i in x['index_num']], axis = 1).values,\n",
    "    corpus=corpus,\n",
    "    dictionary=Dictionary.from_corpus(corpus, id2word),\n",
    "    coherence='c_npmi',\n",
    ")\n",
    "\n",
    "cm.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "55fc9984",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010575433373172134,\n",
       " -0.08864524531280475,\n",
       " -0.29738757225699636,\n",
       " -0.12675700414382224,\n",
       " -0.00046802004933244707,\n",
       " -0.2907925052459905,\n",
       " -0.24912661770649866,\n",
       " 0.09824602830557984,\n",
       " -0.07246353833620021,\n",
       " 0.5782802533523147,\n",
       " -0.08915202093578214,\n",
       " 0.19697654636731932,\n",
       " -0.209023759286811,\n",
       " -0.09121954098799176,\n",
       " -0.02777727205228651,\n",
       " 0.039474022414436885,\n",
       " -0.0730726645682248,\n",
       " -0.036998336957972625,\n",
       " -0.09971504504233546,\n",
       " -0.13076801562550727]"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "e32d457e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-580-adbe7533365f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index_num'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "c8f727a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "618d06fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenToLogNormal(nn.Module):\n",
    "    def __init__(self, hidden_size, num_topics):\n",
    "        super().__init__()\n",
    "        self.fcmu = nn.Linear(hidden_size, num_topics)\n",
    "        self.fclv = nn.Linear(hidden_size, num_topics)\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics)\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics)\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        mu = self.bnmu(self.fcmu(hidden))\n",
    "        lv = self.bnlv(self.fclv(hidden))\n",
    "        dist = LogNormal(mu, (0.5 * lv).exp())\n",
    "        return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "827eeeee",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    return prior\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "0d7e2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import kl_divergence\n",
    "class NTM(nn.Module):\n",
    "    \"\"\"NTM that keeps track of output\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, normal, h_to_z, topics):\n",
    "        super(NTM, self).__init__()\n",
    "        self.hidden = hidden\n",
    "        self.normal = normal\n",
    "        self.h_to_z = h_to_z\n",
    "        self.topics = topics\n",
    "        self.output = None\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "    def forward(self, x, n_sample=1):\n",
    "        h = self.hidden(x)\n",
    "        h = self.drop(h)\n",
    "        posterior = self.normal(h)\n",
    "        prior = standard_prior_like(posterior)\n",
    "        #identify how far it is away from normal distribution\n",
    "        kld = kl_divergence(posterior, prior).mean()\n",
    "        #print(kld.shape)\n",
    "        rec_loss = 0\n",
    "        for i in range(n_sample):\n",
    "            #reparametrician trick\n",
    "            z = posterior.rsample().to(device)\n",
    "            #decode\n",
    "            \n",
    "            z = z / z.sum(1, keepdim=True)\n",
    "            self.output = z\n",
    "            #print(z)\n",
    "            #z = self.drop(z)\n",
    "            #get log probability for reconstruction loss\n",
    "            log_prob = self.topics(z)\n",
    "            rec_loss = rec_loss - (log_prob * x).sum(dim=-1)\n",
    "        #average reconstruction loss\n",
    "        rec_loss = rec_loss / n_sample\n",
    "        #print(rec_loss.shape)\n",
    "        minus_elbo = rec_loss + kld\n",
    "\n",
    "        return {\n",
    "            'loss': minus_elbo,\n",
    "            'minus_elbo': minus_elbo,\n",
    "            'rec_loss': rec_loss,\n",
    "            'kld': kld\n",
    "        }\n",
    "\n",
    "    def get_topics(self):\n",
    "        return self.topics.get_topics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "147b355d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n",
      "nan nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-489-5e1872291827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-411-ae882023c876>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, X, batch_size, epoch, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from torch.distributions import LogNormal, Dirichlet\n",
    "#labels = [[120], [1527], [1646], [2047], [727], [1624], [36], [32], [26], [92], [907], [652]]\n",
    "numb_embeddings = 20\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1],  64], nn.ReLU)\n",
    "normal = HiddenToLogNormal(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "# embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "# embedding.weight.requires_grad=False\n",
    "embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "embedding.weight.requires_grad=False\n",
    "topics = EmbTopic(embedding = embedding,\n",
    "                  k = numb_embeddings, normalize = False)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "model = NTM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics\n",
    "            \n",
    "        \n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.002, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=epochs)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer, scheduler)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "11292d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102, 102, 111, 617, 85, 52, 81, 56, 26, 27, 79, 28, 65, 76, 41, 84, 41, 53, 16, 24]\n",
      "[176, 68, 63, 524, 352, 180, 109, 16, 28, 12, 3, 6, 4, 3, 4, 2, 6, 3, 5, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.09370688740316248,\n",
       " 0.031428944734180744,\n",
       " 0.08330680250451024,\n",
       " 0.03507299395058063)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prior(softmax_top, indexes)\n",
    "emb = model.topics.get_topics().cpu().detach().numpy()\n",
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:25] ] for i in range(20)]\n",
    "data_batch = torch.from_numpy(X.toarray()).float()\n",
    "model.cpu()\n",
    "h = model.hidden(data_batch)\n",
    "h = model.drop(h)\n",
    "mu = model.normal(h)\n",
    "z = mu.mean\n",
    "labels_pred = torch.argmax(z, 1).numpy()\n",
    "labels_true = df['class'].values\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20, random_state=0).fit(z.detach().numpy())\n",
    "top_purity(labels_true, labels_pred), metrics.normalized_mutual_info_score(labels_true, labels_pred), top_purity(labels_true, kmeans.labels_), metrics.normalized_mutual_info_score(labels_true, kmeans.labels_)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "2501c288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 794,\n",
       "         18: 924,\n",
       "         7: 1669,\n",
       "         17: 5989,\n",
       "         14: 1185,\n",
       "         9: 725,\n",
       "         15: 1019,\n",
       "         11: 470,\n",
       "         6: 211,\n",
       "         13: 292,\n",
       "         4: 831,\n",
       "         5: 371,\n",
       "         0: 643,\n",
       "         2: 914,\n",
       "         19: 441,\n",
       "         8: 1096,\n",
       "         16: 523,\n",
       "         12: 351,\n",
       "         1: 195,\n",
       "         3: 203})"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "collections.Counter(labels_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9017d",
   "metadata": {},
   "source": [
    "# AG News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9962df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.read_csv('https://raw.githubusercontent.com/yumeng5/WeSTClass/master/agnews/dataset.csv', names = ['class', 'documents', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b5afe654",
   "metadata": {},
   "outputs": [],
   "source": [
    "D['clean_text'] = D['documents'] + D['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "82796994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d2cdfb02ab4dfa8301c2cf205fe08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D['clean_text'] = D.swifter.apply(lambda x: ' '.join(remove_stopWords(x['documents'] + x['text'])).split(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d36410de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate word 2 vec models\n",
    "w2vmodel = Word2Vec(D['clean_text'].values,vector_size=200, window=10, negative = 5, min_count= 10)\n",
    "\n",
    "#generate input for the model\n",
    "vocab = list(set([j for i in init_docs for j in i if j in w2vmodel.wv]))\n",
    "vocab = [''] + vocab\n",
    "word_track = {i: ind for ind, i in enumerate(vocab)}\n",
    "index_track = {ind: i for ind, i in enumerate(vocab)}\n",
    "#pad the input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccfd19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0082b8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca1af4a0",
   "metadata": {},
   "source": [
    "# RCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8efd978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1 = fetch_rcv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cannot use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4910ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup dataset NYT and Arxiv\n",
    "\n",
    "#https://github.com/yumeng5/JoSH/tree/master/datasets\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
