{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c074a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "#change to any directory you have to store the repo\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "\n",
    "from src.features.metric import diversity, get_topic_coherence\n",
    "from models.atten_model import MODEL_ATT_COMP\n",
    "import swifter\n",
    "from src.models.utils import sinkhorn_torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2dc5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import swifter\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "stop_words = get_stop_words('en')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import pickle \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "from models.NVDM import sinkhorn_torch, negative_sampling_prior, optimal_transport_prior,  NormalParameter, get_mlp, EmbTopic, NSSTM, OTETM\n",
    "from src.models.utils import get_wordnet_pos, remove_stopWords, get_emb, generate_emb, train, kld_normal, get_common_words, generate_bow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739664e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee1a2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a074f6e7d84b0981a1436bf2261062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data import\n",
    "news = pd.read_csv('https://raw.githubusercontent.com/yumeng5/WeSTClass/master/agnews/dataset.csv', \n",
    "                   error_bad_lines=False,\n",
    "                   names = ['class', 'title', 'description'])\n",
    "news['text'] = news.swifter.apply(lambda x: ' '.join(remove_stopWords(x['title'] + x['description'])), axis=1)\n",
    "news['clean_text']  = news.apply(lambda x: x['text'].split(), axis = 1)\n",
    "#get clean data\n",
    "common_words_ct = Counter([j for i in news['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 100)\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)}\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)}\n",
    "news['index_num'] = news.swifter.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa27fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change it to any location you save your embeddings\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/agnews/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = news, common_words = common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list  = [['government', 'military', 'war'],\n",
    " ['basketball', 'football', 'athlete'],\n",
    " ['stock', 'market', 'industry'],\n",
    " ['computer', 'telescope', 'software']]\n",
    "labels = [[word_track[j] for j in i] for i in seed_topic_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2f0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_embeddings = len(seed_topic_list) + 1\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1], 512, 64], nn.ReLU)\n",
    "normal = NormalParameter(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "embedding.weight.requires_grad=False\n",
    "topics = EmbTopic(embedding = embedding,\n",
    "                  k = numb_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NSSTM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics,\n",
    "            #prior_penalty = 1, \n",
    "            diversity_penalty = 0, \n",
    "            iter1 = 15,\n",
    "            iter2 = 20,\n",
    "            beta = 0.25,\n",
    "            gamma = 0.25,\n",
    "            index = labels\n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=40)\n",
    "\n",
    "epochs = 25\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267deab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
