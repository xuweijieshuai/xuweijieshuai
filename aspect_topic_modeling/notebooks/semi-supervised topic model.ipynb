{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2f4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "#change to any directory you have to store the repo\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/github/aspect_topic_modeling')\n",
    "\n",
    "from src.features.metric import diversity, get_topic_coherence\n",
    "from models.atten_model import MODEL_ATT_COMP\n",
    "import swifter\n",
    "from src.models.utils import sinkhorn_torch\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "577c6a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import swifter\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "stop_words = get_stop_words('en')\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "import pickle \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "from sklearn.metrics import f1_score\n",
    "from torch import nn, optim\n",
    "from torch.nn import init\n",
    "from models.NVDM import topic_covariance_penalty, sinkhorn_torch, NTM, negative_sampling_prior, optimal_transport_prior,  NormalParameter, get_mlp, EmbTopic, NSSTM, OTETM\n",
    "from src.models.utils import get_wordnet_pos, remove_stopWords, get_emb, generate_emb, train, kld_normal, get_common_words, generate_bow\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701d7d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae22393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a074f6e7d84b0981a1436bf2261062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e28e065f3048f896e5ade5f99bc3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data import\n",
    "news = pd.read_csv('https://raw.githubusercontent.com/yumeng5/WeSTClass/master/agnews/dataset.csv', \n",
    "                   error_bad_lines=False,\n",
    "                   names = ['class', 'title', 'description'])\n",
    "news['text'] = news.swifter.apply(lambda x: ' '.join(remove_stopWords(x['title'] + x['description'])), axis=1)\n",
    "news['clean_text']  = news.apply(lambda x: x['text'].split(), axis = 1)\n",
    "#get clean data\n",
    "common_words_ct = Counter([j for i in news['clean_text'].values for j in i])\n",
    "common_words = get_common_words(common_words_ct, ct = 100)\n",
    "word_track = {i: ind for ind, i in enumerate(common_words)}\n",
    "index_track = {ind: i for ind, i in enumerate(common_words)}\n",
    "news['index_num'] = news.swifter.apply(\n",
    "            lambda x: [word_track[i] for i in x['clean_text'] if i in word_track], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6456b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change it to any location you save your embeddings\n",
    "vec = '/home/ec2-user/SageMaker/ORMCorpVoatp/ormcorpvoatp/ormcorpvoatp/data/Spherical-Text-Embedding/datasets/agnews/jose.txt'\n",
    "embed = generate_emb(vec, common_words).cpu()\n",
    "X, indices = generate_bow(df = news, common_words = common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2445f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic_list  = [['government', 'military', 'war'],\n",
    " ['basketball', 'football', 'athlete'],\n",
    " ['stock', 'market', 'industry'],\n",
    " ['computer', 'telescope', 'software']]\n",
    "labels = [[word_track[j] for j in i] for i in seed_topic_list ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "268c1f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSSTM(NTM):\n",
    "    \"\"\"\n",
    "    A class used for negative sampling based semi-supervised topic modeling\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    beta\n",
    "    gamma\n",
    "    diversity_penalty: coefficients for diversity penalty\n",
    "    index: a list of index of keywords\n",
    "    sample\n",
    "    \n",
    "    Methods:\n",
    "    --------\n",
    "    forward(logit)\n",
    "        a disctionary of loss function\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, hidden, normal, h_to_z, topics, diversity_penalty, \n",
    "                 index, beta = 1, gamma = 1, iter1=10, iter2=20, sample = 20):\n",
    "        # h_to_z will output probabilities over topics\n",
    "        super(NSSTM, self).__init__(hidden, normal, h_to_z, topics)\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.diversity_penalty = diversity_penalty\n",
    "        self.index = index\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.iter1 = iter1\n",
    "        self.iter2 = iter2\n",
    "        self.sample = sample\n",
    "    def forward(self, x, n_sample=1, epoch = 0):\n",
    "        stat = super(NSSTM, self).forward(x, n_sample)\n",
    "        loss = stat['rec_loss'] \n",
    "        #penalty is mean - variance standard deviation\n",
    "        if self.index == [] or epoch < self.iter1:\n",
    "            self.ppenalty = 0\n",
    "        else:\n",
    "            self.ppenalty = negative_sampling_prior(self.topics.get_topics(),self.index, self.topics.embedding, epoch,\n",
    "                                                   self.beta, self.gamma, self.iter2, self.sample)\n",
    "        dpenalty, _, _ = topic_covariance_penalty(self.topics.topic_emb)\n",
    "        stat.update({\n",
    "            #loss add some penalty\n",
    "            'loss': loss + self.ppenalty  + dpenalty * self.diversity_penalty,\n",
    "            'penalty': self.ppenalty  + dpenalty * self.diversity_penalty,\n",
    "            'prior': self.ppenalty\n",
    "        })\n",
    "\n",
    "        return stat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7fc946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:25: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162.1897666296725 12.498382521999766 0.0\n",
      "155.44772659334294 32.57520895268617 0.0\n",
      "148.6199530009776 73.15312489263539 0.0\n",
      "144.61395224630198 105.49415720144569 0.0\n",
      "143.23828777321367 124.69790193927822 0.0\n",
      "142.76595254340913 146.85595823503508 0.0\n",
      "142.5907206586175 173.02774246313425 0.0\n",
      "142.50005804196095 198.17312342271623 0.0\n",
      "142.43575299879126 221.75990366986565 0.0\n",
      "142.3849003187883 248.94229230087703 0.0\n",
      "142.7321109405713 269.71429638567764 tensor(5.1672, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.8392647293839 289.7628602310539 tensor(4.9211, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.8390905709663 306.4072947552972 tensor(4.8684, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.82803775811755 322.18561584863073 tensor(4.8394, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.8038824280696 334.1128047463228 tensor(4.8220, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.78704646909668 346.5145993100555 tensor(4.8078, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.82387674490272 354.9600032977204 tensor(-0.1544, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.8344505326326 360.19825487604527 tensor(-0.2109, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.83108328552896 362.2400659101604 tensor(-0.2249, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "142.82702272329757 362.71357034061003 tensor(-0.2263, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "numb_embeddings = len(seed_topic_list) + 1\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "hidden = get_mlp([X.shape[1], 64], nn.ReLU)\n",
    "normal = NormalParameter(64, numb_embeddings)\n",
    "h_to_z = nn.Softmax()\n",
    "embedding = nn.Embedding(X.shape[1], 50)\n",
    "# p1d = (0, 0, 0, 10000 - company1.embeddings.shape[0]) # pad last dim by 1 on each side\n",
    "# out = F.pad(company1.embeddings, p1d, \"constant\", 0)  # effectively zero padding\n",
    "\n",
    "embedding.weight = torch.nn.Parameter(torch.Tensor(embed.float()))\n",
    "embedding.weight.requires_grad=False\n",
    "topics = EmbTopic(embedding = embedding,\n",
    "                  k = numb_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NSSTM(hidden = hidden,\n",
    "            normal = normal,\n",
    "            h_to_z = h_to_z,\n",
    "            topics = topics,\n",
    "            #prior_penalty = 1, \n",
    "            diversity_penalty = 0, \n",
    "            iter1 = 10,\n",
    "            iter2 = 15,\n",
    "            beta = 0.25,\n",
    "            gamma = 0.25,\n",
    "            index = labels\n",
    "            ).to(device).float()\n",
    "# larger hidden size make topics more diverse\n",
    "#num_docs_train = 996318\n",
    "batch_size = 256\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=0.002, \n",
    "                       weight_decay=1.2e-6)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=int(X.shape[0]/batch_size) + 1, epochs=20)\n",
    "\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    train(model, X,  batch_size, epoch, optimizer, scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "73d58f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/ipykernel/__main__.py:12: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the model is 0.8023\n",
      "The Diversity of the model is 0.8400000000000001\n",
      "The F1 macro score of the model is 0.798388240829509\n",
      "0.8023 0.8400000000000001 0.798388240829509 0.9269292310185185\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "emb = model.topics.get_topics().cpu().detach().numpy()\n",
    "topics =  [[index_track[ind] for ind in np.argsort(emb[i])[::-1][:10] ] for i in range(numb_embeddings)]\n",
    "\n",
    "#visualize keywords\n",
    "mapping = [emb[:, i].mean(1).argmax() for i in labels]\n",
    "\n",
    "data_batch = torch.from_numpy(X.toarray()).float()\n",
    "model.cpu()\n",
    "z = model.hidden(data_batch)\n",
    "z, _ = model.normal(z)\n",
    "z = model.h_to_z(z)\n",
    "zz = torch.stack([z[:, i] for i in mapping]).T\n",
    "zz = zz.cpu().detach().numpy()\n",
    "# zz[:,0] = zz[:,0] * np.sqrt(0.219)\n",
    "# zz[:,1] = zz[:,1]* np.sqrt(0.3826)\n",
    "# zz[:,2] = zz[:,2]* np.sqrt(0.3106)\n",
    "# zz[:,3] = zz[:,3] * np.sqrt(0.08724) \n",
    "y_pred = zz.argmax(1)\n",
    "y_true = news['class'].iloc[indices].values - 1\n",
    "accuracy = np.sum(y_pred == y_true)/news.shape[0]\n",
    "coherence_score = get_topic_coherence(X.toarray(), seed_topic_list, word_track)\n",
    "diversity_score = np.mean(diversity(topics))\n",
    "macro = f1_score(y_true, y_pred, average='macro')\n",
    "micro = f1_score(y_true, y_pred, average='micro')\n",
    "print('The accuracy of the model is ' + str(accuracy))\n",
    "print('The Diversity of the model is ' + str(diversity_score) )\n",
    "print('The F1 macro score of the model is ' + str(macro))\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "y_real = enc.fit_transform(y_true.reshape(-1, 1)).toarray()\n",
    "aucroc = roc_auc_score(y_real, zz, multi_class = 'ovo')      \n",
    "print(accuracy, diversity_score, macro, aucroc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "620dfc55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20235707, 0.19664623, 0.18870987, 0.18870987],\n",
       "       [0.20235707, 0.19664623, 0.18870987, 0.18870987],\n",
       "       [0.20235707, 0.19664623, 0.18870987, 0.18870987],\n",
       "       ...,\n",
       "       [0.20235707, 0.19664623, 0.18870987, 0.18870987],\n",
       "       [0.20235707, 0.19664623, 0.18870987, 0.18870987],\n",
       "       [0.20235707, 0.19664623, 0.18870987, 0.18870987]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d7602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
